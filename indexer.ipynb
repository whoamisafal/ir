{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab774061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f6e1d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m json_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_index.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "\n",
    "with open(\"search_index.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            json_data.append(obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error parsing line {i}: {e}\")\n",
    "\n",
    "print(\"Total records:\", len(json_data))\n",
    "print(\"First record:\", json_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9380ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/github',\n",
       " 'title': 'GitHub Â· GitHub',\n",
       " 'description': 'How people build software. GitHub has 531 repositories available. Follow their code on GitHub.',\n",
       " 'keywords': ['github', 'source', 'security'],\n",
       " 'visible_text': \"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see our . GitHub We've verified that the organization controls the domain: Hey, this is us ðŸ‘‹ Yes, we are building GitHub on GitHub. In fact, weâ€™ve been doing this since . That's when we made our first commit. Since then we pushed , opened , submitted roughly across from over . ðŸ¤¯ But that's just us. We are proud to be part of the work of millions of developers, companies and robots across the solar system. ðŸª Yes, ! ðŸ¿ An interconnected community The open source community is the ðŸ’— heart of GitHub and fundamental to how we build software today. See for yourself: Now that we are talking about the important things, â˜ï¸ are you contributing to open source? Yes? Okay, you rock! ðŸŽ¸ If not, we can help you get started! Open source software is made by people just like you. Learn more about . ðŸ¦¦ Contributing to the ecosystem We contribute to the tools ðŸ”§ we rely on to build and run GitHub, while also maintaining ðŸ§™\\u200dâ™‚ï¸ our own open source projects like: ðŸ‘“ Appendix See what's next on our âœ¨ and if you have any suggestions. ðŸ™‡\\u200dâ™‚ï¸ Oh, and by the way, we are always hiring talented, passionate people to . ðŸ™Œ Pinned Open source courseware for Git and GitHub A site to provide non-judgmental guidance on choosing a license for your open source project ðŸ”¬ A Ruby library for carefully refactoring critical paths. GitHub's Online Schema-migration Tool for MySQL GitHub's official MCP Server Repositories Security vulnerability database inclusive of CVEs and GitHub originated security advisories from the world of open source software. An extension for Visual Studio Code that adds rich language support for CodeQL GitHub's official MCP Server CodeQL: the libraries and queries that power security researchers around the world, as well as code scanning in GitHub Advanced Security A web app that creates GitHub-themed collectibles from GitHub profiles Contains curations submitted by the community GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal. A collection of rust algorithms and data structures A month for maintainers to gather & share Golang implementation of a checker for determining if an SPDX ID satisfies an SPDX Expression. Footer Footer navigation\",\n",
       " 'image_urls': ['https://avatars.githubusercontent.com/u/9919?s=200&v=4',\n",
       "  'https://user-images.githubusercontent.com/3369400/133268513-5bfe2f93-4402-42c9-a403-81c9e86934b6.jpeg',\n",
       "  'https://avatars.githubusercontent.com/u/182?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/623?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/2205?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/2501?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/3341?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/4215?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/4619?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/4719?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/6218?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/8599?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10186?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10515?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10998?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/11050?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/11922?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/17516?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/17770?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/23497?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/30761?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/34163?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/57157753?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/144427?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/116213842?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5624255?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5313261?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/1915?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5837277?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5580297?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/1503512?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/1617169?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/170270?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10643?s=70&v=4'],\n",
       " 'internal_links': ['https://github.com/',\n",
       "  'https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgithub',\n",
       "  'https://github.com/github',\n",
       "  'https://github.com/features/copilot',\n",
       "  'https://github.com/features/spark',\n",
       "  'https://github.com/features/models',\n",
       "  'https://github.com/mcp',\n",
       "  'https://github.com/features/actions',\n",
       "  'https://github.com/features/codespaces',\n",
       "  'https://github.com/features/issues',\n",
       "  'https://github.com/features/code-review',\n",
       "  'https://github.com/security/advanced-security',\n",
       "  'https://github.com/security/advanced-security/code-security',\n",
       "  'https://github.com/security/advanced-security/secret-protection',\n",
       "  'https://github.com/why-github',\n",
       "  'https://docs.github.com',\n",
       "  'https://github.com/marketplace',\n",
       "  'https://github.com/features',\n",
       "  'https://github.com/enterprise',\n",
       "  'https://github.com/team',\n",
       "  'https://github.com/enterprise/startups',\n",
       "  'https://github.com/solutions/industry/nonprofits',\n",
       "  'https://github.com/solutions/use-case/app-modernization',\n",
       "  'https://github.com/solutions/use-case/devsecops',\n",
       "  'https://github.com/solutions/use-case/devops',\n",
       "  'https://github.com/solutions/use-case/ci-cd',\n",
       "  'https://github.com/solutions/use-case',\n",
       "  'https://github.com/solutions/industry/healthcare',\n",
       "  'https://github.com/solutions/industry/financial-services',\n",
       "  'https://github.com/solutions/industry/manufacturing',\n",
       "  'https://github.com/solutions/industry/government',\n",
       "  'https://github.com/solutions/industry',\n",
       "  'https://github.com/solutions',\n",
       "  'https://github.com/resources/articles?topic=ai',\n",
       "  'https://github.com/resources/articles?topic=software-development',\n",
       "  'https://github.com/resources/articles?topic=devops',\n",
       "  'https://github.com/resources/articles?topic=security',\n",
       "  'https://github.com/resources/articles',\n",
       "  'https://github.com/customer-stories',\n",
       "  'https://github.com/resources/events',\n",
       "  'https://github.com/resources/whitepapers',\n",
       "  'https://github.com/solutions/executive-insights',\n",
       "  'https://skills.github.com',\n",
       "  'https://docs.github.com',\n",
       "  'https://support.github.com',\n",
       "  'https://github.com/orgs/community/discussions',\n",
       "  'https://github.com/trust-center',\n",
       "  'https://github.com/partners',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://securitylab.github.com',\n",
       "  'https://maintainers.github.com',\n",
       "  'https://github.com/accelerator',\n",
       "  'https://archiveprogram.github.com',\n",
       "  'https://github.com/topics',\n",
       "  'https://github.com/trending',\n",
       "  'https://github.com/collections',\n",
       "  'https://github.com/enterprise',\n",
       "  'https://github.com/security/advanced-security',\n",
       "  'https://github.com/features/copilot/copilot-business',\n",
       "  'https://github.com/premium-support',\n",
       "  'https://github.com/pricing',\n",
       "  'https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax',\n",
       "  'https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax',\n",
       "  'https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgithub',\n",
       "  'https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Corg-login%3E&source=header',\n",
       "  'https://docs.github.com/organizations/managing-organization-settings/verifying-or-approving-a-domain-for-your-organization',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://github.com/orgs/github/followers',\n",
       "  'https://github.com/about',\n",
       "  'https://github.com/github',\n",
       "  'https://github.com/orgs/github/repositories',\n",
       "  'https://github.com/orgs/github/projects',\n",
       "  'https://github.com/orgs/github/packages',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/orgs/github/sponsoring',\n",
       "  'https://github.com/github',\n",
       "  'https://github.com/orgs/github/repositories',\n",
       "  'https://github.com/orgs/github/projects',\n",
       "  'https://github.com/orgs/github/packages',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/orgs/github/sponsoring',\n",
       "  'https://github.com/github/.github/tree/main/profile/README.md',\n",
       "  'https://github.com/readme/featured/nasa-ingenuity-helicopter',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://github.com/cli/cli',\n",
       "  'https://github.com/desktop/desktop',\n",
       "  'https://github.com/git-lfs/git-lfs',\n",
       "  'https://github.com/primer/css',\n",
       "  'https://github.com/github/roadmap',\n",
       "  'https://github.com/github/feedback',\n",
       "  'https://github.com/about/careers',\n",
       "  'https://github.com/rails',\n",
       "  'https://github.com/golang',\n",
       "  'https://github.com/primer',\n",
       "  'https://github.com/reactjs',\n",
       "  'https://github.com/apache/kafka',\n",
       "  'https://github.com/microsoft/vscode',\n",
       "  'https://github.com/rails/rails',\n",
       "  'https://github.com/Homebrew',\n",
       "  'https://github.com/github/docs',\n",
       "  'https://docs.github.com/en/organizations/collaborating-with-groups-in-organizations/customizing-your-organizations-profile',\n",
       "  'https://github.com/github/training-kit',\n",
       "  'https://github.com/github/training-kit/stargazers',\n",
       "  'https://github.com/github/training-kit/forks',\n",
       "  'https://github.com/github/choosealicense.com',\n",
       "  'https://github.com/github/choosealicense.com/stargazers',\n",
       "  'https://github.com/github/choosealicense.com/forks',\n",
       "  'https://github.com/github/scientist',\n",
       "  'https://github.com/github/scientist/stargazers',\n",
       "  'https://github.com/github/scientist/forks',\n",
       "  'https://github.com/github/gh-ost',\n",
       "  'https://github.com/github/gh-ost/stargazers',\n",
       "  'https://github.com/github/gh-ost/forks',\n",
       "  'https://github.com/github/github-mcp-server',\n",
       "  'https://github.com/github/github-mcp-server/stargazers',\n",
       "  'https://github.com/github/github-mcp-server/forks',\n",
       "  'https://github.com/github/advisory-database',\n",
       "  'https://github.com/github/advisory-database/graphs/commit-activity',\n",
       "  'https://github.com/github/advisory-database/stargazers',\n",
       "  'https://github.com/github/advisory-database/forks',\n",
       "  'https://github.com/github/advisory-database/issues',\n",
       "  'https://github.com/github/advisory-database/pulls',\n",
       "  'https://github.com/github/vscode-codeql',\n",
       "  'https://github.com/github/vscode-codeql/graphs/commit-activity',\n",
       "  'https://github.com/github/vscode-codeql/stargazers',\n",
       "  'https://github.com/github/vscode-codeql/forks',\n",
       "  'https://github.com/github/vscode-codeql/issues',\n",
       "  'https://github.com/github/vscode-codeql/issues?q=label%3A%22good+first+issue%22+is%3Aissue+is%3Aopen',\n",
       "  'https://github.com/github/vscode-codeql/pulls',\n",
       "  'https://github.com/github/github-mcp-server',\n",
       "  'https://github.com/github/github-mcp-server/graphs/commit-activity',\n",
       "  'https://github.com/github/github-mcp-server/stargazers',\n",
       "  'https://github.com/github/github-mcp-server/forks',\n",
       "  'https://github.com/github/github-mcp-server/issues',\n",
       "  'https://github.com/github/github-mcp-server/pulls',\n",
       "  'https://github.com/github/codeql',\n",
       "  'https://github.com/github/codeql/graphs/commit-activity',\n",
       "  'https://github.com/github/codeql/stargazers',\n",
       "  'https://github.com/github/codeql/forks',\n",
       "  'https://github.com/github/codeql/issues',\n",
       "  'https://github.com/github/codeql/issues?q=label%3A%22good+first+issue%22+is%3Aissue+is%3Aopen',\n",
       "  'https://github.com/github/codeql/pulls',\n",
       "  'https://github.com/github/octocanvas',\n",
       "  'https://github.com/github/octocanvas/graphs/commit-activity',\n",
       "  'https://github.com/github/octocanvas/stargazers',\n",
       "  'https://github.com/github/octocanvas/forks',\n",
       "  'https://github.com/github/octocanvas/issues',\n",
       "  'https://github.com/github/octocanvas/pulls',\n",
       "  'https://github.com/github/curated-data',\n",
       "  'https://github.com/clearlydefined/curated-data',\n",
       "  'https://github.com/github/curated-data/graphs/commit-activity',\n",
       "  'https://github.com/github/curated-data/stargazers',\n",
       "  'https://github.com/github/curated-data/forks',\n",
       "  'https://github.com/github/curated-data/issues',\n",
       "  'https://github.com/github/curated-data/pulls',\n",
       "  'https://github.com/github/copilot-cli',\n",
       "  'https://github.com/github/copilot-cli/graphs/commit-activity',\n",
       "  'https://github.com/github/copilot-cli/stargazers',\n",
       "  'https://github.com/github/copilot-cli/forks',\n",
       "  'https://github.com/github/copilot-cli/issues',\n",
       "  'https://github.com/github/copilot-cli/pulls',\n",
       "  'https://github.com/github/rust-gems',\n",
       "  'https://github.com/github/rust-gems/graphs/commit-activity',\n",
       "  'https://github.com/github/rust-gems/stargazers',\n",
       "  'https://github.com/github/rust-gems/forks',\n",
       "  'https://github.com/github/rust-gems/issues',\n",
       "  'https://github.com/github/rust-gems/pulls',\n",
       "  'https://github.com/github/maintainermonth',\n",
       "  'https://github.com/github/maintainermonth/graphs/commit-activity',\n",
       "  'https://github.com/github/maintainermonth/stargazers',\n",
       "  'https://github.com/github/maintainermonth/forks',\n",
       "  'https://github.com/github/maintainermonth/issues',\n",
       "  'https://github.com/github/maintainermonth/issues?q=label%3A%22help+wanted%22+is%3Aissue+is%3Aopen',\n",
       "  'https://github.com/github/maintainermonth/pulls',\n",
       "  'https://github.com/github/go-spdx',\n",
       "  'https://github.com/github/go-spdx/graphs/commit-activity',\n",
       "  'https://github.com/github/go-spdx/stargazers',\n",
       "  'https://github.com/github/go-spdx/forks',\n",
       "  'https://github.com/github/go-spdx/issues',\n",
       "  'https://github.com/github/go-spdx/pulls',\n",
       "  'https://github.com/orgs/github/repositories?type=all',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/mtodd',\n",
       "  'https://github.com/jonmagic',\n",
       "  'https://github.com/kjg',\n",
       "  'https://github.com/kdaigle',\n",
       "  'https://github.com/abraham',\n",
       "  'https://github.com/nathos',\n",
       "  'https://github.com/omgitsads',\n",
       "  'https://github.com/tma',\n",
       "  'https://github.com/nickh',\n",
       "  'https://github.com/digitarald',\n",
       "  'https://github.com/look',\n",
       "  'https://github.com/kyanny',\n",
       "  'https://github.com/rubiojr',\n",
       "  'https://github.com/northrup',\n",
       "  'https://github.com/hilli',\n",
       "  'https://github.com/antonio',\n",
       "  'https://github.com/phillmv',\n",
       "  'https://github.com/mahata',\n",
       "  'https://github.com/azizshamim',\n",
       "  'https://github.com/georgedrummond',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/Open-Source-Collective',\n",
       "  'https://github.com/marijnh',\n",
       "  'https://github.com/SAML-Toolkits',\n",
       "  'https://github.com/exercism',\n",
       "  'https://github.com/its-a-feature',\n",
       "  'https://github.com/kivikakk',\n",
       "  'https://github.com/knsv',\n",
       "  'https://github.com/tannerlinsley',\n",
       "  'https://github.com/Homebrew',\n",
       "  'https://github.com/processing',\n",
       "  'https://github.com/sindresorhus',\n",
       "  'https://github.com/zx2c4',\n",
       "  'https://github.com/orgs/github/repositories?language=ruby&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=javascript&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=typescript&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=go&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=python&type=all',\n",
       "  'https://github.com/search?q=topic%3Acustom-elements+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Agithub+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Aweb-components+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Aruby+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Aactions+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://docs.github.com',\n",
       "  'https://github.com',\n",
       "  'https://docs.github.com/site-policy/github-terms/github-terms-of-service',\n",
       "  'https://docs.github.com/site-policy/privacy-policies/github-privacy-statement',\n",
       "  'https://github.com/security',\n",
       "  'https://github.community/',\n",
       "  'https://docs.github.com/',\n",
       "  'https://support.github.com?tags=dotcom-footer'],\n",
       " 'content_hash': '904077296839aa7aefc0022352901add1de7ff3e4aa93424bbd6d66e5d1af4f7',\n",
       " 'crawled_at': '2026-01-18 17:16:01',\n",
       " 'depth': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502dbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GitHub Docs Get started, troubleshoot, and make the most of GitHub. Documentation for new users, developers, administrators, and all of GitHub's products. ['github', 'secure', 'started', 'copilot', 'project'] GitHub Docs Help for wherever you are on your GitHub journey. Get started Collaborative coding GitHub Copilot CI/CD and DevOps Security and quality Client apps Project management Enterprise and teams Developers Community More docs Getting started At the heart of GitHub is an open-source version control system (VCS) called Git. Git is responsible for everything GitHub-related that happens locally on your computer. You can connect to GitHub using the Secure Shell Protocol (SSH), which provides a secure channel over an unsecured network. You can create a repository on GitHub to store and collaborate on your project's files, then manage the repository's name and location. Create sophisticated formatting for your prose and code on GitHub with simple syntax. Popular Pull requests let you propose, review, and merge code changes. Keep your account and data secure with features like two-factor authentication, SSH, and commit signature verification. Use GitHub Copilot to get code suggestions in your editor. Learn to work with your local repositories on your computer and remote repositories hosted on GitHub. Help and support Did you find what you needed? Help us make these docs great! All GitHub docs are open source. See something that's wrong or unclear? Submit a pull request. Still need help? Legal\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = json_data[0]['title']\n",
    "description = json_data[0]['description']\n",
    "keywords = json_data[0]['keywords']\n",
    "visible_text = json_data[0]['visible_text']\n",
    "full_text = f'{title} {description} {keywords} {visible_text}'\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8148e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation & numbers\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords + short tokens + lemmatize\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "\n",
    "    return clean_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cf4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['github', 'doc', 'get', 'started', 'troubleshoot', 'make', 'github', 'documentation', 'new', 'user', 'developer', 'administrator', 'github', 'product', 'github', 'secure', 'started', 'copilot', 'project', 'github', 'doc', 'help', 'wherever', 'github', 'journey']\n",
      "Total tokens: 155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = preprocess_text(full_text)\n",
    "\n",
    "print(\"Sample tokens:\", tokens[:25])\n",
    "print(\"Total tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for 'data': [(0, 105)]\n",
      "Index size: 107\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "doc_id = 0   # change for multiple documents\n",
    "\n",
    "for position, token in enumerate(tokens):\n",
    "    inverted_index[token].append((doc_id, position))\n",
    "\n",
    "# Example\n",
    "print(\"Index for 'data':\", inverted_index.get(\"data\"))\n",
    "print(\"Index size:\", len(inverted_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216228c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'github': [(0, 0),\n",
       "              (0, 6),\n",
       "              (0, 12),\n",
       "              (0, 14),\n",
       "              (0, 19),\n",
       "              (0, 23),\n",
       "              (0, 29),\n",
       "              (0, 46),\n",
       "              (0, 58),\n",
       "              (0, 64),\n",
       "              (0, 77),\n",
       "              (0, 91),\n",
       "              (0, 117),\n",
       "              (0, 131),\n",
       "              (0, 140)],\n",
       "             'doc': [(0, 1), (0, 20), (0, 42), (0, 138), (0, 141)],\n",
       "             'get': [(0, 2), (0, 25), (0, 119)],\n",
       "             'started': [(0, 3), (0, 16), (0, 26), (0, 44)],\n",
       "             'troubleshoot': [(0, 4)],\n",
       "             'make': [(0, 5), (0, 137)],\n",
       "             'documentation': [(0, 7)],\n",
       "             'new': [(0, 8)],\n",
       "             'user': [(0, 9)],\n",
       "             'developer': [(0, 10), (0, 40)],\n",
       "             'administrator': [(0, 11)],\n",
       "             'product': [(0, 13)],\n",
       "             'secure': [(0, 15), (0, 66), (0, 71), (0, 106)],\n",
       "             'copilot': [(0, 17), (0, 30), (0, 118)],\n",
       "             'project': [(0, 18), (0, 36), (0, 80)],\n",
       "             'help': [(0, 21), (0, 132), (0, 136), (0, 153)],\n",
       "             'wherever': [(0, 22)],\n",
       "             'journey': [(0, 24)],\n",
       "             'collaborative': [(0, 27)],\n",
       "             'coding': [(0, 28)],\n",
       "             'devops': [(0, 31)],\n",
       "             'security': [(0, 32)],\n",
       "             'quality': [(0, 33)],\n",
       "             'client': [(0, 34)],\n",
       "             'apps': [(0, 35)],\n",
       "             'management': [(0, 37)],\n",
       "             'enterprise': [(0, 38)],\n",
       "             'team': [(0, 39)],\n",
       "             'community': [(0, 41)],\n",
       "             'getting': [(0, 43)],\n",
       "             'heart': [(0, 45)],\n",
       "             'open': [(0, 47), (0, 142)],\n",
       "             'source': [(0, 48), (0, 143)],\n",
       "             'version': [(0, 49)],\n",
       "             'control': [(0, 50)],\n",
       "             'system': [(0, 51)],\n",
       "             'vcs': [(0, 52)],\n",
       "             'called': [(0, 53)],\n",
       "             'git': [(0, 54), (0, 55)],\n",
       "             'responsible': [(0, 56)],\n",
       "             'everything': [(0, 57)],\n",
       "             'related': [(0, 59)],\n",
       "             'happens': [(0, 60)],\n",
       "             'locally': [(0, 61)],\n",
       "             'computer': [(0, 62), (0, 127)],\n",
       "             'connect': [(0, 63)],\n",
       "             'using': [(0, 65)],\n",
       "             'shell': [(0, 67)],\n",
       "             'protocol': [(0, 68)],\n",
       "             'ssh': [(0, 69), (0, 112)],\n",
       "             'provides': [(0, 70)],\n",
       "             'channel': [(0, 72)],\n",
       "             'unsecured': [(0, 73)],\n",
       "             'network': [(0, 74)],\n",
       "             'create': [(0, 75), (0, 86)],\n",
       "             'repository': [(0, 76), (0, 83), (0, 126), (0, 129)],\n",
       "             'store': [(0, 78)],\n",
       "             'collaborate': [(0, 79)],\n",
       "             'file': [(0, 81)],\n",
       "             'manage': [(0, 82)],\n",
       "             'name': [(0, 84)],\n",
       "             'location': [(0, 85)],\n",
       "             'sophisticated': [(0, 87)],\n",
       "             'formatting': [(0, 88)],\n",
       "             'prose': [(0, 89)],\n",
       "             'code': [(0, 90), (0, 101), (0, 120)],\n",
       "             'simple': [(0, 92)],\n",
       "             'syntax': [(0, 93)],\n",
       "             'popular': [(0, 94)],\n",
       "             'pull': [(0, 95), (0, 149)],\n",
       "             'request': [(0, 96), (0, 150)],\n",
       "             'let': [(0, 97)],\n",
       "             'propose': [(0, 98)],\n",
       "             'review': [(0, 99)],\n",
       "             'merge': [(0, 100)],\n",
       "             'change': [(0, 102)],\n",
       "             'keep': [(0, 103)],\n",
       "             'account': [(0, 104)],\n",
       "             'data': [(0, 105)],\n",
       "             'feature': [(0, 107)],\n",
       "             'like': [(0, 108)],\n",
       "             'two': [(0, 109)],\n",
       "             'factor': [(0, 110)],\n",
       "             'authentication': [(0, 111)],\n",
       "             'commit': [(0, 113)],\n",
       "             'signature': [(0, 114)],\n",
       "             'verification': [(0, 115)],\n",
       "             'use': [(0, 116)],\n",
       "             'suggestion': [(0, 121)],\n",
       "             'editor': [(0, 122)],\n",
       "             'learn': [(0, 123)],\n",
       "             'work': [(0, 124)],\n",
       "             'local': [(0, 125)],\n",
       "             'remote': [(0, 128)],\n",
       "             'hosted': [(0, 130)],\n",
       "             'support': [(0, 133)],\n",
       "             'find': [(0, 134)],\n",
       "             'needed': [(0, 135)],\n",
       "             'great': [(0, 139)],\n",
       "             'see': [(0, 144)],\n",
       "             'something': [(0, 145)],\n",
       "             'wrong': [(0, 146)],\n",
       "             'unclear': [(0, 147)],\n",
       "             'submit': [(0, 148)],\n",
       "             'still': [(0, 151)],\n",
       "             'need': [(0, 152)],\n",
       "             'legal': [(0, 154)]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _on_executor_deleted at 0x0000022A6FB46290>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\periodic_executor.py\", line 274, in _on_executor_deleted\n",
      "    _EXECUTORS.remove(ref)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total records loaded: 16742\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -----------------------\n",
    "# Create data directory\n",
    "# -----------------------\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILE = \"search_index.jsonl\"\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, \"inverted_index.json\")\n",
    "\n",
    "# -----------------------\n",
    "# Load JSONL data\n",
    "# -----------------------\n",
    "json_data = []\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            json_data.append(obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error parsing line {i}: {e}\")\n",
    "\n",
    "print(\"âœ… Total records loaded:\", len(json_data))\n",
    "\n",
    "# -----------------------\n",
    "# NLTK Setup\n",
    "# -----------------------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "\n",
    "# -----------------------\n",
    "# Build Inverted Index\n",
    "# -----------------------\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for doc_id, record in enumerate(json_data):\n",
    "    title = record.get(\"title\", \"\")\n",
    "    description = record.get(\"description\", \"\")\n",
    "    keywords = record.get(\"keywords\", \"\")\n",
    "    visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "    full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "\n",
    "    tokens = preprocess_text(full_text)\n",
    "\n",
    "    # Avoid duplicate doc_id per term\n",
    "    for token in set(tokens):\n",
    "        inverted_index[token].append(doc_id)\n",
    "\n",
    "# Sort postings\n",
    "for token in inverted_index:\n",
    "    inverted_index[token] = sorted(inverted_index[token])\n",
    "\n",
    "# -----------------------\n",
    "# Save Index\n",
    "# -----------------------\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inverted_index, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ“ Index saved in: {OUTPUT_FILE}\")\n",
    "print(f\"ðŸ“š Total indexed terms:\", len(inverted_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from preprocess import preprocess_text\n",
    "# -----------------------\n",
    "# MongoDB Setup\n",
    "# -----------------------\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_database\"]\n",
    "docs_col = db[\"documents\"]\n",
    "index_col = db[\"inverted_index\"]\n",
    "\n",
    "# -----------------------\n",
    "# NLTK Setup\n",
    "# -----------------------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -----------------------\n",
    "# Load JSONL and Insert/Update\n",
    "# -----------------------\n",
    "INPUT_FILE = \"search_index.jsonl\"\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        url = record.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        # Combine all text fields\n",
    "        title = record.get(\"title\", \"\")\n",
    "        description = record.get(\"description\", \"\")\n",
    "        keywords = \" \".join(record.get(\"keywords\", []))\n",
    "        visible_text = record.get(\"visible_text\", \"\")\n",
    "        full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "\n",
    "        # Preprocess tokens\n",
    "        tokens = preprocess_text(full_text)\n",
    "\n",
    "        # Check if document exists\n",
    "        existing_doc = docs_col.find_one({\"url\": url})\n",
    "\n",
    "        if existing_doc:\n",
    "            # Document exists, check content hash\n",
    "            if existing_doc.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                # No changes, skip\n",
    "                continue\n",
    "            else:\n",
    "                # Update document\n",
    "                doc_id = existing_doc[\"_id\"]\n",
    "\n",
    "                # Remove old tokens from inverted index\n",
    "                old_tokens = existing_doc.get(\"tokens\", [])\n",
    "                for token in set(old_tokens):\n",
    "                    index_col.update_one(\n",
    "                        {\"token\": token},\n",
    "                        {\"$pull\": {\"postings\": doc_id}}\n",
    "                    )\n",
    "\n",
    "                # Update document fields\n",
    "                docs_col.update_one(\n",
    "                    {\"_id\": doc_id},\n",
    "                    {\"$set\": {\n",
    "                        \"title\": title,\n",
    "                        \"description\": description,\n",
    "                        \"keywords\": record.get(\"keywords\", []),\n",
    "                        \"visible_text\": visible_text,\n",
    "                        \"image_urls\": record.get(\"image_urls\", []),\n",
    "                        \"content_hash\": record.get(\"content_hash\"),\n",
    "                        \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                        \"depth\": record.get(\"depth\"),\n",
    "                        \"tokens\": tokens\n",
    "                    }}\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Insert new document\n",
    "            doc_id = docs_col.estimated_document_count() + 1\n",
    "            docs_col.insert_one({\n",
    "                \"_id\": doc_id,\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"keywords\": record.get(\"keywords\", []),\n",
    "                \"visible_text\": visible_text,\n",
    "                \"image_urls\": record.get(\"image_urls\", []),\n",
    "                \"content_hash\": record.get(\"content_hash\"),\n",
    "                \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                \"depth\": record.get(\"depth\"),\n",
    "                \"tokens\": tokens\n",
    "            })\n",
    "\n",
    "        # Update inverted index\n",
    "        for token in set(tokens):\n",
    "            index_col.update_one(\n",
    "                {\"token\": token},\n",
    "                {\"$addToSet\": {\"postings\": doc_id}},\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "print(\"âœ… Documents updated/added and inverted index updated!\")\n",
    "print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8645ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting indexing...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 193\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_no, lines \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunked_iterator(f, BATCH_SIZE), \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    100\u001b[0m     future \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39msubmit(process_lines, lines)\n\u001b[1;32m--> 101\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     doc_ops \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    104\u001b[0m     index_ops \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mprocess_lines\u001b[1;34m(lines)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from preprocess import preprocess_text\n",
    "from itertools import islice\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "INPUT_FILE = \"batches/batch_1.json\"\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"ir_database\"\n",
    "\n",
    "DOC_COLLECTION = \"documents\"\n",
    "INDEX_COLLECTION = \"inverted_index_posting_list\"\n",
    "COUNTER_COLLECTION = \"counters\"\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "WORKERS = 8\n",
    "\n",
    "# -----------------------\n",
    "# MongoDB Setup\n",
    "# -----------------------\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "docs_col = db[DOC_COLLECTION]\n",
    "index_col = db[INDEX_COLLECTION]\n",
    "counter_col = db[COUNTER_COLLECTION]\n",
    "\n",
    "# -----------------------\n",
    "# Ensure counter exists\n",
    "# -----------------------\n",
    "counter_col.update_one(\n",
    "    {\"_id\": \"doc_id\"},\n",
    "    {\"$setOnInsert\": {\"seq\": 0}},\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "def get_next_doc_id():\n",
    "    doc = counter_col.find_one_and_update(\n",
    "        {\"_id\": \"doc_id\"},\n",
    "        {\"$inc\": {\"seq\": 1}},\n",
    "        return_document=True\n",
    "    )\n",
    "    return doc[\"seq\"]\n",
    "\n",
    "# -----------------------\n",
    "# Chunk reader\n",
    "# -----------------------\n",
    "def chunked_iterator(file, size):\n",
    "    while True:\n",
    "        chunk = list(islice(file, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "# -----------------------\n",
    "# Worker: preprocess only (CPU safe)\n",
    "# -----------------------\n",
    "def process_lines(lines):\n",
    "    batch = []\n",
    "\n",
    "    for line in lines:\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        url = record.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        title = record.get(\"title\", \"\")\n",
    "        description = record.get(\"description\", \"\")\n",
    "        keywords = \" \".join(record.get(\"keywords\", []))\n",
    "        visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "        full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "        tokens = list(set(preprocess_text(full_text)))\n",
    "\n",
    "        batch.append({\n",
    "            \"url\": url,\n",
    "            \"tokens\": tokens,\n",
    "            \"record\": record\n",
    "        })\n",
    "\n",
    "    return batch\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"ðŸš€ Starting indexing...\\n\")\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f, \\\n",
    "         ThreadPoolExecutor(max_workers=WORKERS) as pool:\n",
    "\n",
    "        for batch_no, lines in enumerate(chunked_iterator(f, BATCH_SIZE), 1):\n",
    "\n",
    "            future = pool.submit(process_lines, lines)\n",
    "            processed = future.result()\n",
    "\n",
    "            doc_ops = []\n",
    "            index_ops = []\n",
    "\n",
    "            for item in processed:\n",
    "                url = item[\"url\"]\n",
    "                tokens = item[\"tokens\"]\n",
    "                record = item[\"record\"]\n",
    "\n",
    "                existing = docs_col.find_one(\n",
    "                    {\"url\": url},\n",
    "                    {\"_id\": 1, \"content_hash\": 1, \"tokens\": 1}\n",
    "                )\n",
    "\n",
    "                # ------------------\n",
    "                # Skip unchanged docs\n",
    "                # ------------------\n",
    "                if existing and existing.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                    continue\n",
    "\n",
    "                # ------------------\n",
    "                # Assign doc_id\n",
    "                # ------------------\n",
    "                if existing:\n",
    "                    doc_id = existing[\"_id\"]\n",
    "\n",
    "                    # Remove old tokens from inverted index\n",
    "                    for old_token in existing.get(\"tokens\", []):\n",
    "                        index_ops.append(\n",
    "                            UpdateOne(\n",
    "                                {\"token\": old_token},\n",
    "                                {\"$pull\": {\"postings\": doc_id}}\n",
    "                            )\n",
    "                        )\n",
    "                else:\n",
    "                    doc_id = get_next_doc_id()\n",
    "\n",
    "                # ------------------\n",
    "                # Upsert document\n",
    "                # ------------------\n",
    "                doc_ops.append(\n",
    "                    UpdateOne(\n",
    "                        {\"_id\": doc_id},\n",
    "                        {\"$set\": {\n",
    "                            \"url\": url,\n",
    "                            \"title\": record.get(\"title\", \"\"),\n",
    "                            \"description\": record.get(\"description\", \"\"),\n",
    "                            \"keywords\": record.get(\"keywords\", []),\n",
    "                            \"visible_text\": record.get(\"visible_text\", \"\"),\n",
    "                            \"image_urls\": record.get(\"image_urls\", []),\n",
    "                            \"content_hash\": record.get(\"content_hash\"),\n",
    "                            \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                            \"depth\": record.get(\"depth\"),\n",
    "                            \"tokens\": tokens,\n",
    "                            \"internal_links\": record.get(\"internal_links\",[]),\n",
    "                        }},\n",
    "                        upsert=True\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # ------------------\n",
    "                # Update inverted index\n",
    "                # ------------------\n",
    "                for token in tokens:\n",
    "                    index_ops.append(\n",
    "                        UpdateOne(\n",
    "                            {\"token\": token},\n",
    "                            {\"$addToSet\": {\"postings\": doc_id}},\n",
    "                            upsert=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            # ------------------\n",
    "            # Bulk write\n",
    "            # ------------------\n",
    "            if doc_ops:\n",
    "                docs_col.bulk_write(doc_ops, ordered=False)\n",
    "\n",
    "            if index_ops:\n",
    "                index_col.bulk_write(index_ops, ordered=False)\n",
    "\n",
    "            print(f\"âœ… Batch {batch_no} indexed ({len(processed)} records)\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Indexing completed!\")\n",
    "    print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "    print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n",
    "\n",
    "# -----------------------\n",
    "# Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_17.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 16742\n",
      "ðŸ“š Total indexed terms: 124275\n",
      "18\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_18.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 17052\n",
      "ðŸ“š Total indexed terms: 125667\n",
      "19\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_19.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 17572\n",
      "ðŸ“š Total indexed terms: 129091\n",
      "20\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_20.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 18499\n",
      "ðŸ“š Total indexed terms: 134129\n",
      "21\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_21.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 19463\n",
      "ðŸ“š Total indexed terms: 138701\n",
      "22\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_22.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 20420\n",
      "ðŸ“š Total indexed terms: 141993\n",
      "23\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_23.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 21249\n",
      "ðŸ“š Total indexed terms: 144755\n",
      "24\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_24.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 21993\n",
      "ðŸ“š Total indexed terms: 146928\n",
      "25\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_25.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 22742\n",
      "ðŸ“š Total indexed terms: 147605\n",
      "26\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_26.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 23704\n",
      "ðŸ“š Total indexed terms: 150970\n",
      "27\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_27.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 24578\n",
      "ðŸ“š Total indexed terms: 153368\n",
      "28\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_28.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 25385\n",
      "ðŸ“š Total indexed terms: 155879\n",
      "29\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_29.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 26305\n",
      "ðŸ“š Total indexed terms: 157161\n",
      "30\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_30.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 27151\n",
      "ðŸ“š Total indexed terms: 159906\n",
      "31\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_31.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 28151\n",
      "ðŸ“š Total indexed terms: 164921\n",
      "32\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_32.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 29085\n",
      "ðŸ“š Total indexed terms: 173292\n",
      "33\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_33.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 30073\n",
      "ðŸ“š Total indexed terms: 175643\n",
      "34\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_34.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 31040\n",
      "ðŸ“š Total indexed terms: 181865\n",
      "35\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 409 records from batches/batch_35.json\n",
      "\n",
      "âœ… Batch 1 indexed (409 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 31442\n",
      "ðŸ“š Total indexed terms: 183785\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "start=17\n",
    "end=36\n",
    "for i in range(start,end):\n",
    "    print(i)\n",
    "    INPUT_FILE = f\"batches/batch_{i}.json\"\n",
    "    MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "    DB_NAME = \"ir_db\"\n",
    "\n",
    "    DOC_COLLECTION = \"documents\"\n",
    "    INDEX_COLLECTION = \"inverted_index\"\n",
    "    COUNTER_COLLECTION = \"counters\"\n",
    "\n",
    "    BATCH_SIZE = 500\n",
    "    WORKERS = 8\n",
    "\n",
    "    # -----------------------\n",
    "    # MongoDB Setup\n",
    "    # -----------------------\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "\n",
    "    docs_col = db[DOC_COLLECTION]\n",
    "    index_col = db[INDEX_COLLECTION]\n",
    "    counter_col = db[COUNTER_COLLECTION]\n",
    "\n",
    "    # -----------------------\n",
    "    # Ensure indexes (run once safely)\n",
    "    # -----------------------\n",
    "    docs_col.create_index(\"url\", unique=True)\n",
    "    index_col.create_index(\"token\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Ensure counter exists\n",
    "    # -----------------------\n",
    "    counter_col.update_one(\n",
    "        {\"_id\": \"doc_id\"},\n",
    "        {\"$setOnInsert\": {\"seq\": 0}},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    def get_next_doc_id():\n",
    "        doc = counter_col.find_one_and_update(\n",
    "            {\"_id\": \"doc_id\"},\n",
    "            {\"$inc\": {\"seq\": 1}},\n",
    "            return_document=True\n",
    "        )\n",
    "        return doc[\"seq\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # Chunk list helper\n",
    "    # -----------------------\n",
    "    def chunk_list(data, size):\n",
    "        for i in range(0, len(data), size):\n",
    "            yield data[i:i + size]\n",
    "\n",
    "    # -----------------------\n",
    "    # Worker: preprocess records\n",
    "    # -----------------------\n",
    "    def process_records(records):\n",
    "        batch = []\n",
    "\n",
    "        for record in records:\n",
    "            url = record.get(\"url\")\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            title = record.get(\"title\", \"\")\n",
    "            description = record.get(\"description\", \"\")\n",
    "            keywords = \" \".join(record.get(\"keywords\", []))\n",
    "            visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "            full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "            tokens = list(set(preprocess_text(full_text)))\n",
    "\n",
    "            batch.append({\n",
    "                \"url\": url,\n",
    "                \"tokens\": tokens,\n",
    "                \"record\": record\n",
    "            })\n",
    "\n",
    "        return batch\n",
    "\n",
    "    # -----------------------\n",
    "    # Main pipeline\n",
    "    # -----------------------\n",
    "    def main():\n",
    "        print(\"ðŸš€ Starting indexing...\\n\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Load JSON batch file\n",
    "        # -----------------------\n",
    "        with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_records = json.load(f)\n",
    "\n",
    "        print(f\"ðŸ“¦ Loaded {len(all_records)} records from {INPUT_FILE}\\n\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Process in batches\n",
    "        # -----------------------\n",
    "        with ThreadPoolExecutor(max_workers=WORKERS) as pool:\n",
    "\n",
    "            for batch_no, records in enumerate(chunk_list(all_records, BATCH_SIZE), 1):\n",
    "\n",
    "                # Optional: Deduplicate URLs inside this batch\n",
    "                unique = {}\n",
    "                for r in records:\n",
    "                    if \"url\" in r:\n",
    "                        unique[r[\"url\"]] = r\n",
    "                records = list(unique.values())\n",
    "\n",
    "                future = pool.submit(process_records, records)\n",
    "                processed = future.result()\n",
    "\n",
    "                doc_ops = []\n",
    "                index_ops = []\n",
    "\n",
    "                for item in processed:\n",
    "                    url = item[\"url\"]\n",
    "                    tokens = item[\"tokens\"]\n",
    "                    record = item[\"record\"]\n",
    "\n",
    "                    existing = docs_col.find_one(\n",
    "                        {\"url\": url},\n",
    "                        {\"_id\": 1, \"content_hash\": 1, \"tokens\": 1}\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Skip unchanged docs\n",
    "                    # ------------------\n",
    "                    if existing and existing.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                        continue\n",
    "\n",
    "                    # ------------------\n",
    "                    # Assign doc_id\n",
    "                    # ------------------\n",
    "                    if existing:\n",
    "                        doc_id = existing[\"_id\"]\n",
    "\n",
    "                        # Remove old tokens from inverted index\n",
    "                        for old_token in existing.get(\"tokens\", []):\n",
    "                            index_ops.append(\n",
    "                                UpdateOne(\n",
    "                                    {\"token\": old_token},\n",
    "                                    {\"$pull\": {\"postings\": doc_id}}\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        doc_id = get_next_doc_id()\n",
    "\n",
    "                    # ------------------\n",
    "                    # âœ… FIXED: Upsert document using URL\n",
    "                    # ------------------\n",
    "                    doc_ops.append(\n",
    "                        UpdateOne(\n",
    "                            {\"url\": url},     # âœ… match by URL (not _id)\n",
    "                            {\"$set\": {\n",
    "                                \"_id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"title\": record.get(\"title\", \"\"),\n",
    "                                \"description\": record.get(\"description\", \"\"),\n",
    "                                \"keywords\": record.get(\"keywords\", []),\n",
    "                                \"visible_text\": record.get(\"visible_text\", \"\"),\n",
    "                                \"image_urls\": record.get(\"image_urls\", []),\n",
    "                                \"content_hash\": record.get(\"content_hash\"),\n",
    "                                \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                                \"depth\": record.get(\"depth\"),\n",
    "                                \"tokens\": tokens,\n",
    "                                \"internal_links\": record.get(\"internal_links\", []),\n",
    "                            }},\n",
    "                            upsert=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Update inverted index\n",
    "                    # ------------------\n",
    "                    for token in tokens:\n",
    "                        index_ops.append(\n",
    "                            UpdateOne(\n",
    "                                {\"token\": token},\n",
    "                                {\"$addToSet\": {\"postings\": doc_id}},\n",
    "                                upsert=True\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # ------------------\n",
    "                # Bulk write to MongoDB\n",
    "                # ------------------\n",
    "                if doc_ops:\n",
    "                    docs_col.bulk_write(doc_ops, ordered=False)\n",
    "\n",
    "                if index_ops:\n",
    "                    index_col.bulk_write(index_ops, ordered=False)\n",
    "\n",
    "                print(f\"âœ… Batch {batch_no} indexed ({len(processed)} records)\")\n",
    "\n",
    "        print(\"\\nðŸŽ‰ Indexing completed!\")\n",
    "        print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "        print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n",
    "\n",
    "    # -----------------------\n",
    "    # Run\n",
    "    # -----------------------\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165854f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved batches/batch_1.json (1000 records)\n",
      "âœ… Saved batches/batch_2.json (1000 records)\n",
      "âœ… Saved batches/batch_3.json (1000 records)\n",
      "âœ… Saved batches/batch_4.json (1000 records)\n",
      "âœ… Saved batches/batch_5.json (1000 records)\n",
      "âœ… Saved batches/batch_6.json (1000 records)\n",
      "âœ… Saved batches/batch_7.json (1000 records)\n",
      "âœ… Saved batches/batch_8.json (1000 records)\n",
      "âœ… Saved batches/batch_9.json (1000 records)\n",
      "âœ… Saved batches/batch_10.json (1000 records)\n",
      "âœ… Saved batches/batch_11.json (1000 records)\n",
      "âœ… Saved batches/batch_12.json (1000 records)\n",
      "âœ… Saved batches/batch_13.json (1000 records)\n",
      "âœ… Saved batches/batch_14.json (1000 records)\n",
      "âœ… Saved batches/batch_15.json (1000 records)\n",
      "âœ… Saved batches/batch_16.json (1000 records)\n",
      "âœ… Saved batches/batch_17.json (1000 records)\n",
      "âœ… Saved batches/batch_18.json (1000 records)\n",
      "âœ… Saved batches/batch_19.json (1000 records)\n",
      "âœ… Saved batches/batch_20.json (1000 records)\n",
      "âœ… Saved batches/batch_21.json (1000 records)\n",
      "âœ… Saved batches/batch_22.json (1000 records)\n",
      "âœ… Saved batches/batch_23.json (1000 records)\n",
      "âœ… Saved batches/batch_24.json (1000 records)\n",
      "âœ… Saved batches/batch_25.json (1000 records)\n",
      "âœ… Saved batches/batch_26.json (1000 records)\n",
      "âœ… Saved batches/batch_27.json (1000 records)\n",
      "âœ… Saved batches/batch_28.json (1000 records)\n",
      "âœ… Saved batches/batch_29.json (1000 records)\n",
      "âœ… Saved batches/batch_30.json (1000 records)\n",
      "âœ… Saved batches/batch_31.json (1000 records)\n",
      "âœ… Saved batches/batch_32.json (1000 records)\n",
      "âœ… Saved batches/batch_33.json (1000 records)\n",
      "âœ… Saved batches/batch_34.json (1000 records)\n",
      "âœ… Saved batches/batch_35.json (409 records)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"search_index.jsonl\"\n",
    "OUTPUT_DIR = \"batches\"\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "batch = []\n",
    "batch_id = 1\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            batch.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ Skipping invalid JSON at line {line_num}\")\n",
    "            continue\n",
    "\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            output_path = f\"{OUTPUT_DIR}/batch_{batch_id}.json\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                json.dump(batch, out, indent=2)\n",
    "\n",
    "            print(f\"âœ… Saved {output_path} ({len(batch)} records)\")\n",
    "            batch.clear()\n",
    "            batch_id += 1\n",
    "\n",
    "# Save remaining records\n",
    "if batch:\n",
    "    output_path = f\"{OUTPUT_DIR}/batch_{batch_id}.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(batch, out, indent=2)\n",
    "\n",
    "    print(f\"âœ… Saved {output_path} ({len(batch)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cfe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting indexing...\n",
      "\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_17.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_18.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_19.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (499 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_20.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_21.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_22.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (499 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_23.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (499 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_24.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 226\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 213\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    210\u001b[0m                 docs_col\u001b[38;5;241m.\u001b[39mbulk_write(doc_ops, ordered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m index_ops:\n\u001b[1;32m--> 213\u001b[0m                 \u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m indexed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m docs)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸŽ‰ Indexing completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\_csot.py:125\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\collection.py:791\u001b[0m, in \u001b[0;36mCollection.bulk_write\u001b[1;34m(self, requests, ordered, bypass_document_validation, session, comment, let)\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid request\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m write_concern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_concern_for(session)\n\u001b[1;32m--> 791\u001b[0m bulk_api_result \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINSERT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bulk_api_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BulkWriteResult(bulk_api_result, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:751\u001b[0m, in \u001b[0;36m_Bulk.execute\u001b[1;34m(self, write_concern, session, operation)\u001b[0m\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:604\u001b[0m, in \u001b[0;36m_Bulk.execute_command\u001b[1;34m(self, generator, write_concern, session, operation)\u001b[0m\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_command(\n\u001b[0;32m    594\u001b[0m         generator,\n\u001b[0;32m    595\u001b[0m         write_concern,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m         full_result,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    603\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39mdatabase\u001b[38;5;241m.\u001b[39mclient\n\u001b[1;32m--> 604\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_retryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable_bulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteErrors\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m full_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteConcernErrors\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    614\u001b[0m     _raise_bulk_write_error(full_result)\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2083\u001b[0m, in \u001b[0;36mMongoClient._retryable_write\u001b[1;34m(self, retryable, func, session, operation, bulk, operation_id)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute an operation with consecutive retries if possible\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03mReturns func()'s return value on success. On error retries the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;124;03m:param bulk: bulk abstraction to execute operations in bulk, defaults to None\u001b[39;00m\n\u001b[0;32m   2081\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m-> 2083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_with_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:1968\u001b[0m, in \u001b[0;36mMongoClient._retry_with_session\u001b[1;34m(self, retryable, func, session, bulk, operation, operation_id)\u001b[0m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;66;03m# Ensure that the options supports retry_writes and there is a valid session not in\u001b[39;00m\n\u001b[0;32m   1964\u001b[0m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this txn.\u001b[39;00m\n\u001b[0;32m   1965\u001b[0m retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[0;32m   1966\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_writes \u001b[38;5;129;01mand\u001b[39;00m session \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction\n\u001b[0;32m   1967\u001b[0m )\n\u001b[1;32m-> 1968\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1974\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\_csot.py:125\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2014\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[1;34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[0m\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;129m@_csot\u001b[39m\u001b[38;5;241m.\u001b[39mapply\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_retry_internal\u001b[39m(\n\u001b[0;32m   1979\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1988\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1989\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m \n\u001b[0;32m   1992\u001b[0m \u001b[38;5;124;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;124;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2007\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2009\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2763\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error(check_csot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[0;32m   2765\u001b[0m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[0;32m   2766\u001b[0m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[0;32m   2767\u001b[0m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[0;32m   2768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\mongo_client.py:2895\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2887\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying:\n\u001b[0;32m   2888\u001b[0m             _debug_log(\n\u001b[0;32m   2889\u001b[0m                 _COMMAND_LOGGER,\n\u001b[0;32m   2890\u001b[0m                 message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying write attempt number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attempt_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2893\u001b[0m                 operationId\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operation_id,\n\u001b[0;32m   2894\u001b[0m             )\n\u001b[1;32m-> 2895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2896\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   2897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retryable:\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:593\u001b[0m, in \u001b[0;36m_Bulk.execute_command.<locals>.retryable_bulk\u001b[1;34m(session, conn, retryable)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretryable_bulk\u001b[39m(\n\u001b[0;32m    591\u001b[0m     session: Optional[ClientSession], conn: Connection, retryable: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    592\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:538\u001b[0m, in \u001b[0;36m_Bulk._execute_command\u001b[1;34m(self, generator, write_concern, session, conn, op_id, retryable, full_result, final_write_concern)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# Run as many ops as possible in one command.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_concern\u001b[38;5;241m.\u001b[39macknowledged:\n\u001b[1;32m--> 538\u001b[0m     result, to_send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbwc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;66;03m# Retryable writeConcernErrors halt the execution of this run.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m     wce \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteConcernError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:462\u001b[0m, in \u001b[0;36m_Bulk._execute_batch\u001b[1;34m(self, bwc, cmd, ops, client)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     request_id, msg, to_send \u001b[38;5;241m=\u001b[39m bwc\u001b[38;5;241m.\u001b[39mbatch_command(cmd, ops)\n\u001b[1;32m--> 462\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbwc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, to_send\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\helpers.py:47\u001b[0m, in \u001b[0;36m_handle_reauth.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msynchronous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\bulk.py:274\u001b[0m, in \u001b[0;36m_Bulk.write_command\u001b[1;34m(self, bwc, cmd, request_id, msg, docs, client)\u001b[0m\n\u001b[0;32m    272\u001b[0m     bwc\u001b[38;5;241m.\u001b[39m_start(cmd, request_id, docs)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mbwc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbwc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodec\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    275\u001b[0m     duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m bwc\u001b[38;5;241m.\u001b[39mstart_time\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _COMMAND_LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\pool.py:486\u001b[0m, in \u001b[0;36mConnection.write_command\u001b[1;34m(self, request_id, msg, codec_options)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send \"insert\" etc. command, returning response as a dict.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03mCan raise ConnectionFailure or OperationFailure.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m:param msg: bytes, the command message.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_message(msg, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 486\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m result \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39mcommand_response(codec_options)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# Raises NotPrimaryError or OperationFailure.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\pool.py:454\u001b[0m, in \u001b[0;36mConnection.receive_message\u001b[1;34m(self, request_id)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Catch KeyboardInterrupt, CancelledError, etc. and cleanup.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\synchronous\\pool.py:451\u001b[0m, in \u001b[0;36mConnection.receive_message\u001b[1;34m(self, request_id)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Receive a raw BSON message or raise ConnectionFailure.\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03mIf any exception is raised, the socket is closed.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_message_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Catch KeyboardInterrupt, CancelledError, etc. and cleanup.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\network_layer.py:759\u001b[0m, in \u001b[0;36mreceive_message\u001b[1;34m(conn, request_id, max_message_size)\u001b[0m\n\u001b[0;32m    757\u001b[0m         deadline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# Ignore the response's request id.\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m length, _, response_to, op_code \u001b[38;5;241m=\u001b[39m _UNPACK_HEADER(\u001b[43mreceive_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;66;03m# No request_id for exhaust cursor \"getMore\".\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\network_layer.py:343\u001b[0m, in \u001b[0;36mreceive_data\u001b[1;34m(conn, length, deadline)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;66;03m# Use the legacy wait_for_read cancellation approach on PyPy due to PYTHON-5011.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# also use it on Windows due to PYTHON-5405\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _PYPY \u001b[38;5;129;01mor\u001b[39;00m _WINDOWS:\n\u001b[1;32m--> 343\u001b[0m         \u001b[43mwait_for_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _csot\u001b[38;5;241m.\u001b[39mget_timeout() \u001b[38;5;129;01mand\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m             conn\u001b[38;5;241m.\u001b[39mset_conn_timeout(\u001b[38;5;28mmax\u001b[39m(deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic(), \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\network_layer.py:316\u001b[0m, in \u001b[0;36mwait_for_read\u001b[1;34m(conn, deadline)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m _POLL_TIMEOUT\n\u001b[1;32m--> 316\u001b[0m     readable \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_checker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcancel_context\u001b[38;5;241m.\u001b[39mcancelled:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _OperationCancelled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation cancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\socket_checker.py:74\u001b[0m, in \u001b[0;36mSocketChecker.select\u001b[1;34m(self, sock, read, write, timeout)\u001b[0m\n\u001b[0;32m     72\u001b[0m rlist \u001b[38;5;241m=\u001b[39m [sock] \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m     73\u001b[0m wlist \u001b[38;5;241m=\u001b[39m [sock] \u001b[38;5;28;01mif\u001b[39;00m write \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m---> 74\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msock\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# select returns a 3-tuple of lists of objects that are\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# ready: subsets of the first three arguments. Return\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# True if any of the lists are not empty.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(res)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from preprocess import preprocess_text\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "start = 17\n",
    "end = 36\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"ir_db\"\n",
    "\n",
    "DOC_COLLECTION = \"documents_1\"\n",
    "INDEX_COLLECTION = \"inverted_index_tf\"\n",
    "COUNTER_COLLECTION = \"counters_1\"\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "WORKERS = 8\n",
    "\n",
    "# -----------------------\n",
    "# MongoDB Setup\n",
    "# -----------------------\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "docs_col = db[DOC_COLLECTION]\n",
    "index_col = db[INDEX_COLLECTION]\n",
    "counter_col = db[COUNTER_COLLECTION]\n",
    "\n",
    "# -----------------------\n",
    "# Ensure indexes (run once safely)\n",
    "# -----------------------\n",
    "docs_col.create_index(\"url\", unique=True)\n",
    "index_col.create_index(\"token\")\n",
    "\n",
    "# -----------------------\n",
    "# Ensure counter exists\n",
    "# -----------------------\n",
    "counter_col.update_one(\n",
    "    {\"_id\": \"doc_id\"},\n",
    "    {\"$setOnInsert\": {\"seq\": 0}},\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "def get_next_doc_id():\n",
    "    doc = counter_col.find_one_and_update(\n",
    "        {\"_id\": \"doc_id\"},\n",
    "        {\"$inc\": {\"seq\": 1}},\n",
    "        return_document=True\n",
    "    )\n",
    "    return doc[\"seq\"]\n",
    "\n",
    "# -----------------------\n",
    "# Chunk list helper\n",
    "# -----------------------\n",
    "def chunk_list(data, size):\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i + size]\n",
    "\n",
    "# -----------------------\n",
    "# Worker: preprocess records\n",
    "# -----------------------\n",
    "def process_records(records):\n",
    "    batch = []\n",
    "\n",
    "    for record in records:\n",
    "        url = record.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        title = record.get(\"title\", \"\")\n",
    "        description = record.get(\"description\", \"\")\n",
    "        keywords = \" \".join(record.get(\"keywords\", []))\n",
    "        visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "        full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "\n",
    "        token_list = preprocess_text(full_text)     # keep duplicates\n",
    "        tf_map = Counter(token_list)                # count TF\n",
    "\n",
    "        batch.append({\n",
    "            \"url\": url,\n",
    "            \"tf_map\": tf_map,\n",
    "            \"record\": record\n",
    "        })\n",
    "\n",
    "    return batch\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "\n",
    "    print(\"ðŸš€ Starting indexing...\\n\")\n",
    "\n",
    "    for i in range(start, end):\n",
    "        INPUT_FILE = f\"batches/batch_{i}.json\"\n",
    "        print(f\"\\nðŸ“‚ Processing: {INPUT_FILE}\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Load JSON batch file\n",
    "        # -----------------------\n",
    "        with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_records = json.load(f)\n",
    "\n",
    "        print(f\"ðŸ“¦ Loaded {len(all_records)} records\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Process in batches\n",
    "        # -----------------------\n",
    "        with ThreadPoolExecutor(max_workers=WORKERS) as pool:\n",
    "\n",
    "            for batch_no, records in enumerate(chunk_list(all_records, BATCH_SIZE), 1):\n",
    "\n",
    "                # Deduplicate URLs inside batch\n",
    "                unique = {}\n",
    "                for r in records:\n",
    "                    if \"url\" in r:\n",
    "                        unique[r[\"url\"]] = r\n",
    "                records = list(unique.values())\n",
    "\n",
    "                future = pool.submit(process_records, records)\n",
    "                processed = future.result()\n",
    "\n",
    "                doc_ops = []\n",
    "                index_ops = []\n",
    "\n",
    "                for item in processed:\n",
    "                    url = item[\"url\"]\n",
    "                    tf_map = item[\"tf_map\"]\n",
    "                    tokens = list(tf_map.keys())\n",
    "                    record = item[\"record\"]\n",
    "\n",
    "                    existing = docs_col.find_one(\n",
    "                        {\"url\": url},\n",
    "                        {\"_id\": 1, \"content_hash\": 1, \"tokens\": 1}\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Skip unchanged docs\n",
    "                    # ------------------\n",
    "                    if existing and existing.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                        continue\n",
    "\n",
    "                    # ------------------\n",
    "                    # Assign doc_id\n",
    "                    # ------------------\n",
    "                    if existing:\n",
    "                        doc_id = existing[\"_id\"]\n",
    "\n",
    "                        # Remove old postings from inverted index\n",
    "                        for old_token in existing.get(\"tokens\", []):\n",
    "                            index_ops.append(\n",
    "                                UpdateOne(\n",
    "                                    {\"token\": old_token},\n",
    "                                    {\"$pull\": {\"postings\": {\"doc_id\": doc_id}}}\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        doc_id = get_next_doc_id()\n",
    "\n",
    "                    # ------------------\n",
    "                    # Upsert document\n",
    "                    # ------------------\n",
    "                    doc_ops.append(\n",
    "                        UpdateOne(\n",
    "                            {\"url\": url},\n",
    "                            {\"$set\": {\n",
    "                                \"_id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"title\": record.get(\"title\", \"\"),\n",
    "                                \"description\": record.get(\"description\", \"\"),\n",
    "                                \"keywords\": record.get(\"keywords\", []),\n",
    "                                \"visible_text\": record.get(\"visible_text\", \"\"),\n",
    "                                \"image_urls\": record.get(\"image_urls\", []),\n",
    "                                \"content_hash\": record.get(\"content_hash\"),\n",
    "                                \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                                \"depth\": record.get(\"depth\"),\n",
    "                                \"tokens\": tokens,          # unique tokens only\n",
    "                                \"internal_links\": record.get(\"internal_links\", []),\n",
    "                            }},\n",
    "                            upsert=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Update inverted index with TF\n",
    "                    # ------------------\n",
    "                    for token, tf in tf_map.items():\n",
    "                        index_ops.append(\n",
    "                            UpdateOne(\n",
    "                                {\"token\": token},\n",
    "                                {\"$addToSet\": {\n",
    "                                    \"postings\": {\n",
    "                                        \"doc_id\": doc_id,\n",
    "                                        \"tf\": tf\n",
    "                                    }\n",
    "                                }},\n",
    "                                upsert=True\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # ------------------\n",
    "                # Bulk write to MongoDB\n",
    "                # ------------------\n",
    "                if doc_ops:\n",
    "                    docs_col.bulk_write(doc_ops, ordered=False)\n",
    "\n",
    "                if index_ops:\n",
    "                    index_col.bulk_write(index_ops, ordered=False)\n",
    "\n",
    "                print(f\"âœ… Batch {batch_no} indexed ({len(processed)} docs)\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Indexing completed!\")\n",
    "    print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "    print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f442a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
