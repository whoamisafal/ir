{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab774061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "\n",
    "with open(\"search_index.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            json_data.append(obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error parsing line {i}: {e}\")\n",
    "\n",
    "print(\"Total records:\", len(json_data))\n",
    "print(\"First record:\", json_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9380ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/github',\n",
       " 'title': 'GitHub Â· GitHub',\n",
       " 'description': 'How people build software. GitHub has 531 repositories available. Follow their code on GitHub.',\n",
       " 'keywords': ['github', 'source', 'security'],\n",
       " 'visible_text': \"Navigation Menu Search code, repositories, users, issues, pull requests... Provide feedback We read every piece of feedback, and take your input very seriously. Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see our . GitHub We've verified that the organization controls the domain: Hey, this is us ðŸ‘‹ Yes, we are building GitHub on GitHub. In fact, weâ€™ve been doing this since . That's when we made our first commit. Since then we pushed , opened , submitted roughly across from over . ðŸ¤¯ But that's just us. We are proud to be part of the work of millions of developers, companies and robots across the solar system. ðŸª Yes, ! ðŸ¿ An interconnected community The open source community is the ðŸ’— heart of GitHub and fundamental to how we build software today. See for yourself: Now that we are talking about the important things, â˜ï¸ are you contributing to open source? Yes? Okay, you rock! ðŸŽ¸ If not, we can help you get started! Open source software is made by people just like you. Learn more about . ðŸ¦¦ Contributing to the ecosystem We contribute to the tools ðŸ”§ we rely on to build and run GitHub, while also maintaining ðŸ§™\\u200dâ™‚ï¸ our own open source projects like: ðŸ‘“ Appendix See what's next on our âœ¨ and if you have any suggestions. ðŸ™‡\\u200dâ™‚ï¸ Oh, and by the way, we are always hiring talented, passionate people to . ðŸ™Œ Pinned Open source courseware for Git and GitHub A site to provide non-judgmental guidance on choosing a license for your open source project ðŸ”¬ A Ruby library for carefully refactoring critical paths. GitHub's Online Schema-migration Tool for MySQL GitHub's official MCP Server Repositories Security vulnerability database inclusive of CVEs and GitHub originated security advisories from the world of open source software. An extension for Visual Studio Code that adds rich language support for CodeQL GitHub's official MCP Server CodeQL: the libraries and queries that power security researchers around the world, as well as code scanning in GitHub Advanced Security A web app that creates GitHub-themed collectibles from GitHub profiles Contains curations submitted by the community GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal. A collection of rust algorithms and data structures A month for maintainers to gather & share Golang implementation of a checker for determining if an SPDX ID satisfies an SPDX Expression. Footer Footer navigation\",\n",
       " 'image_urls': ['https://avatars.githubusercontent.com/u/9919?s=200&v=4',\n",
       "  'https://user-images.githubusercontent.com/3369400/133268513-5bfe2f93-4402-42c9-a403-81c9e86934b6.jpeg',\n",
       "  'https://avatars.githubusercontent.com/u/182?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/623?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/2205?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/2501?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/3341?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/4215?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/4619?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/4719?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/6218?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/8599?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10186?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10515?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10998?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/11050?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/11922?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/17516?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/17770?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/23497?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/30761?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/34163?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/57157753?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/144427?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/116213842?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5624255?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5313261?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/1915?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5837277?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/5580297?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/1503512?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/1617169?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/170270?s=70&v=4',\n",
       "  'https://avatars.githubusercontent.com/u/10643?s=70&v=4'],\n",
       " 'internal_links': ['https://github.com/',\n",
       "  'https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgithub',\n",
       "  'https://github.com/github',\n",
       "  'https://github.com/features/copilot',\n",
       "  'https://github.com/features/spark',\n",
       "  'https://github.com/features/models',\n",
       "  'https://github.com/mcp',\n",
       "  'https://github.com/features/actions',\n",
       "  'https://github.com/features/codespaces',\n",
       "  'https://github.com/features/issues',\n",
       "  'https://github.com/features/code-review',\n",
       "  'https://github.com/security/advanced-security',\n",
       "  'https://github.com/security/advanced-security/code-security',\n",
       "  'https://github.com/security/advanced-security/secret-protection',\n",
       "  'https://github.com/why-github',\n",
       "  'https://docs.github.com',\n",
       "  'https://github.com/marketplace',\n",
       "  'https://github.com/features',\n",
       "  'https://github.com/enterprise',\n",
       "  'https://github.com/team',\n",
       "  'https://github.com/enterprise/startups',\n",
       "  'https://github.com/solutions/industry/nonprofits',\n",
       "  'https://github.com/solutions/use-case/app-modernization',\n",
       "  'https://github.com/solutions/use-case/devsecops',\n",
       "  'https://github.com/solutions/use-case/devops',\n",
       "  'https://github.com/solutions/use-case/ci-cd',\n",
       "  'https://github.com/solutions/use-case',\n",
       "  'https://github.com/solutions/industry/healthcare',\n",
       "  'https://github.com/solutions/industry/financial-services',\n",
       "  'https://github.com/solutions/industry/manufacturing',\n",
       "  'https://github.com/solutions/industry/government',\n",
       "  'https://github.com/solutions/industry',\n",
       "  'https://github.com/solutions',\n",
       "  'https://github.com/resources/articles?topic=ai',\n",
       "  'https://github.com/resources/articles?topic=software-development',\n",
       "  'https://github.com/resources/articles?topic=devops',\n",
       "  'https://github.com/resources/articles?topic=security',\n",
       "  'https://github.com/resources/articles',\n",
       "  'https://github.com/customer-stories',\n",
       "  'https://github.com/resources/events',\n",
       "  'https://github.com/resources/whitepapers',\n",
       "  'https://github.com/solutions/executive-insights',\n",
       "  'https://skills.github.com',\n",
       "  'https://docs.github.com',\n",
       "  'https://support.github.com',\n",
       "  'https://github.com/orgs/community/discussions',\n",
       "  'https://github.com/trust-center',\n",
       "  'https://github.com/partners',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://securitylab.github.com',\n",
       "  'https://maintainers.github.com',\n",
       "  'https://github.com/accelerator',\n",
       "  'https://archiveprogram.github.com',\n",
       "  'https://github.com/topics',\n",
       "  'https://github.com/trending',\n",
       "  'https://github.com/collections',\n",
       "  'https://github.com/enterprise',\n",
       "  'https://github.com/security/advanced-security',\n",
       "  'https://github.com/features/copilot/copilot-business',\n",
       "  'https://github.com/premium-support',\n",
       "  'https://github.com/pricing',\n",
       "  'https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax',\n",
       "  'https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax',\n",
       "  'https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgithub',\n",
       "  'https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Corg-login%3E&source=header',\n",
       "  'https://docs.github.com/organizations/managing-organization-settings/verifying-or-approving-a-domain-for-your-organization',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://github.com/orgs/github/followers',\n",
       "  'https://github.com/about',\n",
       "  'https://github.com/github',\n",
       "  'https://github.com/orgs/github/repositories',\n",
       "  'https://github.com/orgs/github/projects',\n",
       "  'https://github.com/orgs/github/packages',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/orgs/github/sponsoring',\n",
       "  'https://github.com/github',\n",
       "  'https://github.com/orgs/github/repositories',\n",
       "  'https://github.com/orgs/github/projects',\n",
       "  'https://github.com/orgs/github/packages',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/orgs/github/sponsoring',\n",
       "  'https://github.com/github/.github/tree/main/profile/README.md',\n",
       "  'https://github.com/readme/featured/nasa-ingenuity-helicopter',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://github.com/cli/cli',\n",
       "  'https://github.com/desktop/desktop',\n",
       "  'https://github.com/git-lfs/git-lfs',\n",
       "  'https://github.com/primer/css',\n",
       "  'https://github.com/github/roadmap',\n",
       "  'https://github.com/github/feedback',\n",
       "  'https://github.com/about/careers',\n",
       "  'https://github.com/rails',\n",
       "  'https://github.com/golang',\n",
       "  'https://github.com/primer',\n",
       "  'https://github.com/reactjs',\n",
       "  'https://github.com/apache/kafka',\n",
       "  'https://github.com/microsoft/vscode',\n",
       "  'https://github.com/rails/rails',\n",
       "  'https://github.com/Homebrew',\n",
       "  'https://github.com/github/docs',\n",
       "  'https://docs.github.com/en/organizations/collaborating-with-groups-in-organizations/customizing-your-organizations-profile',\n",
       "  'https://github.com/github/training-kit',\n",
       "  'https://github.com/github/training-kit/stargazers',\n",
       "  'https://github.com/github/training-kit/forks',\n",
       "  'https://github.com/github/choosealicense.com',\n",
       "  'https://github.com/github/choosealicense.com/stargazers',\n",
       "  'https://github.com/github/choosealicense.com/forks',\n",
       "  'https://github.com/github/scientist',\n",
       "  'https://github.com/github/scientist/stargazers',\n",
       "  'https://github.com/github/scientist/forks',\n",
       "  'https://github.com/github/gh-ost',\n",
       "  'https://github.com/github/gh-ost/stargazers',\n",
       "  'https://github.com/github/gh-ost/forks',\n",
       "  'https://github.com/github/github-mcp-server',\n",
       "  'https://github.com/github/github-mcp-server/stargazers',\n",
       "  'https://github.com/github/github-mcp-server/forks',\n",
       "  'https://github.com/github/advisory-database',\n",
       "  'https://github.com/github/advisory-database/graphs/commit-activity',\n",
       "  'https://github.com/github/advisory-database/stargazers',\n",
       "  'https://github.com/github/advisory-database/forks',\n",
       "  'https://github.com/github/advisory-database/issues',\n",
       "  'https://github.com/github/advisory-database/pulls',\n",
       "  'https://github.com/github/vscode-codeql',\n",
       "  'https://github.com/github/vscode-codeql/graphs/commit-activity',\n",
       "  'https://github.com/github/vscode-codeql/stargazers',\n",
       "  'https://github.com/github/vscode-codeql/forks',\n",
       "  'https://github.com/github/vscode-codeql/issues',\n",
       "  'https://github.com/github/vscode-codeql/issues?q=label%3A%22good+first+issue%22+is%3Aissue+is%3Aopen',\n",
       "  'https://github.com/github/vscode-codeql/pulls',\n",
       "  'https://github.com/github/github-mcp-server',\n",
       "  'https://github.com/github/github-mcp-server/graphs/commit-activity',\n",
       "  'https://github.com/github/github-mcp-server/stargazers',\n",
       "  'https://github.com/github/github-mcp-server/forks',\n",
       "  'https://github.com/github/github-mcp-server/issues',\n",
       "  'https://github.com/github/github-mcp-server/pulls',\n",
       "  'https://github.com/github/codeql',\n",
       "  'https://github.com/github/codeql/graphs/commit-activity',\n",
       "  'https://github.com/github/codeql/stargazers',\n",
       "  'https://github.com/github/codeql/forks',\n",
       "  'https://github.com/github/codeql/issues',\n",
       "  'https://github.com/github/codeql/issues?q=label%3A%22good+first+issue%22+is%3Aissue+is%3Aopen',\n",
       "  'https://github.com/github/codeql/pulls',\n",
       "  'https://github.com/github/octocanvas',\n",
       "  'https://github.com/github/octocanvas/graphs/commit-activity',\n",
       "  'https://github.com/github/octocanvas/stargazers',\n",
       "  'https://github.com/github/octocanvas/forks',\n",
       "  'https://github.com/github/octocanvas/issues',\n",
       "  'https://github.com/github/octocanvas/pulls',\n",
       "  'https://github.com/github/curated-data',\n",
       "  'https://github.com/clearlydefined/curated-data',\n",
       "  'https://github.com/github/curated-data/graphs/commit-activity',\n",
       "  'https://github.com/github/curated-data/stargazers',\n",
       "  'https://github.com/github/curated-data/forks',\n",
       "  'https://github.com/github/curated-data/issues',\n",
       "  'https://github.com/github/curated-data/pulls',\n",
       "  'https://github.com/github/copilot-cli',\n",
       "  'https://github.com/github/copilot-cli/graphs/commit-activity',\n",
       "  'https://github.com/github/copilot-cli/stargazers',\n",
       "  'https://github.com/github/copilot-cli/forks',\n",
       "  'https://github.com/github/copilot-cli/issues',\n",
       "  'https://github.com/github/copilot-cli/pulls',\n",
       "  'https://github.com/github/rust-gems',\n",
       "  'https://github.com/github/rust-gems/graphs/commit-activity',\n",
       "  'https://github.com/github/rust-gems/stargazers',\n",
       "  'https://github.com/github/rust-gems/forks',\n",
       "  'https://github.com/github/rust-gems/issues',\n",
       "  'https://github.com/github/rust-gems/pulls',\n",
       "  'https://github.com/github/maintainermonth',\n",
       "  'https://github.com/github/maintainermonth/graphs/commit-activity',\n",
       "  'https://github.com/github/maintainermonth/stargazers',\n",
       "  'https://github.com/github/maintainermonth/forks',\n",
       "  'https://github.com/github/maintainermonth/issues',\n",
       "  'https://github.com/github/maintainermonth/issues?q=label%3A%22help+wanted%22+is%3Aissue+is%3Aopen',\n",
       "  'https://github.com/github/maintainermonth/pulls',\n",
       "  'https://github.com/github/go-spdx',\n",
       "  'https://github.com/github/go-spdx/graphs/commit-activity',\n",
       "  'https://github.com/github/go-spdx/stargazers',\n",
       "  'https://github.com/github/go-spdx/forks',\n",
       "  'https://github.com/github/go-spdx/issues',\n",
       "  'https://github.com/github/go-spdx/pulls',\n",
       "  'https://github.com/orgs/github/repositories?type=all',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/mtodd',\n",
       "  'https://github.com/jonmagic',\n",
       "  'https://github.com/kjg',\n",
       "  'https://github.com/kdaigle',\n",
       "  'https://github.com/abraham',\n",
       "  'https://github.com/nathos',\n",
       "  'https://github.com/omgitsads',\n",
       "  'https://github.com/tma',\n",
       "  'https://github.com/nickh',\n",
       "  'https://github.com/digitarald',\n",
       "  'https://github.com/look',\n",
       "  'https://github.com/kyanny',\n",
       "  'https://github.com/rubiojr',\n",
       "  'https://github.com/northrup',\n",
       "  'https://github.com/hilli',\n",
       "  'https://github.com/antonio',\n",
       "  'https://github.com/phillmv',\n",
       "  'https://github.com/mahata',\n",
       "  'https://github.com/azizshamim',\n",
       "  'https://github.com/georgedrummond',\n",
       "  'https://github.com/orgs/github/people',\n",
       "  'https://github.com/Open-Source-Collective',\n",
       "  'https://github.com/marijnh',\n",
       "  'https://github.com/SAML-Toolkits',\n",
       "  'https://github.com/exercism',\n",
       "  'https://github.com/its-a-feature',\n",
       "  'https://github.com/kivikakk',\n",
       "  'https://github.com/knsv',\n",
       "  'https://github.com/tannerlinsley',\n",
       "  'https://github.com/Homebrew',\n",
       "  'https://github.com/processing',\n",
       "  'https://github.com/sindresorhus',\n",
       "  'https://github.com/zx2c4',\n",
       "  'https://github.com/orgs/github/repositories?language=ruby&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=javascript&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=typescript&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=go&type=all',\n",
       "  'https://github.com/orgs/github/repositories?language=python&type=all',\n",
       "  'https://github.com/search?q=topic%3Acustom-elements+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Agithub+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Aweb-components+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Aruby+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/search?q=topic%3Aactions+org%3Agithub+fork%3Atrue&type=repositories',\n",
       "  'https://github.com/sponsors',\n",
       "  'https://docs.github.com',\n",
       "  'https://github.com',\n",
       "  'https://docs.github.com/site-policy/github-terms/github-terms-of-service',\n",
       "  'https://docs.github.com/site-policy/privacy-policies/github-privacy-statement',\n",
       "  'https://github.com/security',\n",
       "  'https://github.community/',\n",
       "  'https://docs.github.com/',\n",
       "  'https://support.github.com?tags=dotcom-footer'],\n",
       " 'content_hash': '904077296839aa7aefc0022352901add1de7ff3e4aa93424bbd6d66e5d1af4f7',\n",
       " 'crawled_at': '2026-01-18 17:16:01',\n",
       " 'depth': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502dbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GitHub Docs Get started, troubleshoot, and make the most of GitHub. Documentation for new users, developers, administrators, and all of GitHub's products. ['github', 'secure', 'started', 'copilot', 'project'] GitHub Docs Help for wherever you are on your GitHub journey. Get started Collaborative coding GitHub Copilot CI/CD and DevOps Security and quality Client apps Project management Enterprise and teams Developers Community More docs Getting started At the heart of GitHub is an open-source version control system (VCS) called Git. Git is responsible for everything GitHub-related that happens locally on your computer. You can connect to GitHub using the Secure Shell Protocol (SSH), which provides a secure channel over an unsecured network. You can create a repository on GitHub to store and collaborate on your project's files, then manage the repository's name and location. Create sophisticated formatting for your prose and code on GitHub with simple syntax. Popular Pull requests let you propose, review, and merge code changes. Keep your account and data secure with features like two-factor authentication, SSH, and commit signature verification. Use GitHub Copilot to get code suggestions in your editor. Learn to work with your local repositories on your computer and remote repositories hosted on GitHub. Help and support Did you find what you needed? Help us make these docs great! All GitHub docs are open source. See something that's wrong or unclear? Submit a pull request. Still need help? Legal\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = json_data[0]['title']\n",
    "description = json_data[0]['description']\n",
    "keywords = json_data[0]['keywords']\n",
    "visible_text = json_data[0]['visible_text']\n",
    "full_text = f'{title} {description} {keywords} {visible_text}'\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8148e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation & numbers\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove stopwords + short tokens + lemmatize\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "\n",
    "    return clean_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cf4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: ['github', 'doc', 'get', 'started', 'troubleshoot', 'make', 'github', 'documentation', 'new', 'user', 'developer', 'administrator', 'github', 'product', 'github', 'secure', 'started', 'copilot', 'project', 'github', 'doc', 'help', 'wherever', 'github', 'journey']\n",
      "Total tokens: 155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = preprocess_text(full_text)\n",
    "\n",
    "print(\"Sample tokens:\", tokens[:25])\n",
    "print(\"Total tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for 'data': [(0, 105)]\n",
      "Index size: 107\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "doc_id = 0   # change for multiple documents\n",
    "\n",
    "for position, token in enumerate(tokens):\n",
    "    inverted_index[token].append((doc_id, position))\n",
    "\n",
    "# Example\n",
    "print(\"Index for 'data':\", inverted_index.get(\"data\"))\n",
    "print(\"Index size:\", len(inverted_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216228c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'github': [(0, 0),\n",
       "              (0, 6),\n",
       "              (0, 12),\n",
       "              (0, 14),\n",
       "              (0, 19),\n",
       "              (0, 23),\n",
       "              (0, 29),\n",
       "              (0, 46),\n",
       "              (0, 58),\n",
       "              (0, 64),\n",
       "              (0, 77),\n",
       "              (0, 91),\n",
       "              (0, 117),\n",
       "              (0, 131),\n",
       "              (0, 140)],\n",
       "             'doc': [(0, 1), (0, 20), (0, 42), (0, 138), (0, 141)],\n",
       "             'get': [(0, 2), (0, 25), (0, 119)],\n",
       "             'started': [(0, 3), (0, 16), (0, 26), (0, 44)],\n",
       "             'troubleshoot': [(0, 4)],\n",
       "             'make': [(0, 5), (0, 137)],\n",
       "             'documentation': [(0, 7)],\n",
       "             'new': [(0, 8)],\n",
       "             'user': [(0, 9)],\n",
       "             'developer': [(0, 10), (0, 40)],\n",
       "             'administrator': [(0, 11)],\n",
       "             'product': [(0, 13)],\n",
       "             'secure': [(0, 15), (0, 66), (0, 71), (0, 106)],\n",
       "             'copilot': [(0, 17), (0, 30), (0, 118)],\n",
       "             'project': [(0, 18), (0, 36), (0, 80)],\n",
       "             'help': [(0, 21), (0, 132), (0, 136), (0, 153)],\n",
       "             'wherever': [(0, 22)],\n",
       "             'journey': [(0, 24)],\n",
       "             'collaborative': [(0, 27)],\n",
       "             'coding': [(0, 28)],\n",
       "             'devops': [(0, 31)],\n",
       "             'security': [(0, 32)],\n",
       "             'quality': [(0, 33)],\n",
       "             'client': [(0, 34)],\n",
       "             'apps': [(0, 35)],\n",
       "             'management': [(0, 37)],\n",
       "             'enterprise': [(0, 38)],\n",
       "             'team': [(0, 39)],\n",
       "             'community': [(0, 41)],\n",
       "             'getting': [(0, 43)],\n",
       "             'heart': [(0, 45)],\n",
       "             'open': [(0, 47), (0, 142)],\n",
       "             'source': [(0, 48), (0, 143)],\n",
       "             'version': [(0, 49)],\n",
       "             'control': [(0, 50)],\n",
       "             'system': [(0, 51)],\n",
       "             'vcs': [(0, 52)],\n",
       "             'called': [(0, 53)],\n",
       "             'git': [(0, 54), (0, 55)],\n",
       "             'responsible': [(0, 56)],\n",
       "             'everything': [(0, 57)],\n",
       "             'related': [(0, 59)],\n",
       "             'happens': [(0, 60)],\n",
       "             'locally': [(0, 61)],\n",
       "             'computer': [(0, 62), (0, 127)],\n",
       "             'connect': [(0, 63)],\n",
       "             'using': [(0, 65)],\n",
       "             'shell': [(0, 67)],\n",
       "             'protocol': [(0, 68)],\n",
       "             'ssh': [(0, 69), (0, 112)],\n",
       "             'provides': [(0, 70)],\n",
       "             'channel': [(0, 72)],\n",
       "             'unsecured': [(0, 73)],\n",
       "             'network': [(0, 74)],\n",
       "             'create': [(0, 75), (0, 86)],\n",
       "             'repository': [(0, 76), (0, 83), (0, 126), (0, 129)],\n",
       "             'store': [(0, 78)],\n",
       "             'collaborate': [(0, 79)],\n",
       "             'file': [(0, 81)],\n",
       "             'manage': [(0, 82)],\n",
       "             'name': [(0, 84)],\n",
       "             'location': [(0, 85)],\n",
       "             'sophisticated': [(0, 87)],\n",
       "             'formatting': [(0, 88)],\n",
       "             'prose': [(0, 89)],\n",
       "             'code': [(0, 90), (0, 101), (0, 120)],\n",
       "             'simple': [(0, 92)],\n",
       "             'syntax': [(0, 93)],\n",
       "             'popular': [(0, 94)],\n",
       "             'pull': [(0, 95), (0, 149)],\n",
       "             'request': [(0, 96), (0, 150)],\n",
       "             'let': [(0, 97)],\n",
       "             'propose': [(0, 98)],\n",
       "             'review': [(0, 99)],\n",
       "             'merge': [(0, 100)],\n",
       "             'change': [(0, 102)],\n",
       "             'keep': [(0, 103)],\n",
       "             'account': [(0, 104)],\n",
       "             'data': [(0, 105)],\n",
       "             'feature': [(0, 107)],\n",
       "             'like': [(0, 108)],\n",
       "             'two': [(0, 109)],\n",
       "             'factor': [(0, 110)],\n",
       "             'authentication': [(0, 111)],\n",
       "             'commit': [(0, 113)],\n",
       "             'signature': [(0, 114)],\n",
       "             'verification': [(0, 115)],\n",
       "             'use': [(0, 116)],\n",
       "             'suggestion': [(0, 121)],\n",
       "             'editor': [(0, 122)],\n",
       "             'learn': [(0, 123)],\n",
       "             'work': [(0, 124)],\n",
       "             'local': [(0, 125)],\n",
       "             'remote': [(0, 128)],\n",
       "             'hosted': [(0, 130)],\n",
       "             'support': [(0, 133)],\n",
       "             'find': [(0, 134)],\n",
       "             'needed': [(0, 135)],\n",
       "             'great': [(0, 139)],\n",
       "             'see': [(0, 144)],\n",
       "             'something': [(0, 145)],\n",
       "             'wrong': [(0, 146)],\n",
       "             'unclear': [(0, 147)],\n",
       "             'submit': [(0, 148)],\n",
       "             'still': [(0, 151)],\n",
       "             'need': [(0, 152)],\n",
       "             'legal': [(0, 154)]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _on_executor_deleted at 0x0000022A6FB46290>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ir_mini_projecct\\IR\\venv\\lib\\site-packages\\pymongo\\periodic_executor.py\", line 274, in _on_executor_deleted\n",
      "    _EXECUTORS.remove(ref)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total records loaded: 16742\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -----------------------\n",
    "# Create data directory\n",
    "# -----------------------\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILE = \"search_index.jsonl\"\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, \"inverted_index.json\")\n",
    "\n",
    "# -----------------------\n",
    "# Load JSONL data\n",
    "# -----------------------\n",
    "json_data = []\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            json_data.append(obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Error parsing line {i}: {e}\")\n",
    "\n",
    "print(\"âœ… Total records loaded:\", len(json_data))\n",
    "\n",
    "# -----------------------\n",
    "# NLTK Setup\n",
    "# -----------------------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "\n",
    "# -----------------------\n",
    "# Build Inverted Index\n",
    "# -----------------------\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for doc_id, record in enumerate(json_data):\n",
    "    title = record.get(\"title\", \"\")\n",
    "    description = record.get(\"description\", \"\")\n",
    "    keywords = record.get(\"keywords\", \"\")\n",
    "    visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "    full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "\n",
    "    tokens = preprocess_text(full_text)\n",
    "\n",
    "    # Avoid duplicate doc_id per term\n",
    "    for token in set(tokens):\n",
    "        inverted_index[token].append(doc_id)\n",
    "\n",
    "# Sort postings\n",
    "for token in inverted_index:\n",
    "    inverted_index[token] = sorted(inverted_index[token])\n",
    "\n",
    "# -----------------------\n",
    "# Save Index\n",
    "# -----------------------\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inverted_index, f, indent=2)\n",
    "\n",
    "print(f\"ðŸ“ Index saved in: {OUTPUT_FILE}\")\n",
    "print(f\"ðŸ“š Total indexed terms:\", len(inverted_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from preprocess import preprocess_text\n",
    "# -----------------------\n",
    "# MongoDB Setup\n",
    "# -----------------------\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_database\"]\n",
    "docs_col = db[\"documents\"]\n",
    "index_col = db[\"inverted_index\"]\n",
    "\n",
    "# -----------------------\n",
    "# NLTK Setup\n",
    "# -----------------------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -----------------------\n",
    "# Load JSONL and Insert/Update\n",
    "# -----------------------\n",
    "INPUT_FILE = \"search_index.jsonl\"\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        url = record.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        # Combine all text fields\n",
    "        title = record.get(\"title\", \"\")\n",
    "        description = record.get(\"description\", \"\")\n",
    "        keywords = \" \".join(record.get(\"keywords\", []))\n",
    "        visible_text = record.get(\"visible_text\", \"\")\n",
    "        full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "\n",
    "        # Preprocess tokens\n",
    "        tokens = preprocess_text(full_text)\n",
    "\n",
    "        # Check if document exists\n",
    "        existing_doc = docs_col.find_one({\"url\": url})\n",
    "\n",
    "        if existing_doc:\n",
    "            # Document exists, check content hash\n",
    "            if existing_doc.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                # No changes, skip\n",
    "                continue\n",
    "            else:\n",
    "                # Update document\n",
    "                doc_id = existing_doc[\"_id\"]\n",
    "\n",
    "                # Remove old tokens from inverted index\n",
    "                old_tokens = existing_doc.get(\"tokens\", [])\n",
    "                for token in set(old_tokens):\n",
    "                    index_col.update_one(\n",
    "                        {\"token\": token},\n",
    "                        {\"$pull\": {\"postings\": doc_id}}\n",
    "                    )\n",
    "\n",
    "                # Update document fields\n",
    "                docs_col.update_one(\n",
    "                    {\"_id\": doc_id},\n",
    "                    {\"$set\": {\n",
    "                        \"title\": title,\n",
    "                        \"description\": description,\n",
    "                        \"keywords\": record.get(\"keywords\", []),\n",
    "                        \"visible_text\": visible_text,\n",
    "                        \"image_urls\": record.get(\"image_urls\", []),\n",
    "                        \"content_hash\": record.get(\"content_hash\"),\n",
    "                        \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                        \"depth\": record.get(\"depth\"),\n",
    "                        \"tokens\": tokens\n",
    "                    }}\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Insert new document\n",
    "            doc_id = docs_col.estimated_document_count() + 1\n",
    "            docs_col.insert_one({\n",
    "                \"_id\": doc_id,\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"keywords\": record.get(\"keywords\", []),\n",
    "                \"visible_text\": visible_text,\n",
    "                \"image_urls\": record.get(\"image_urls\", []),\n",
    "                \"content_hash\": record.get(\"content_hash\"),\n",
    "                \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                \"depth\": record.get(\"depth\"),\n",
    "                \"tokens\": tokens\n",
    "            })\n",
    "\n",
    "        # Update inverted index\n",
    "        for token in set(tokens):\n",
    "            index_col.update_one(\n",
    "                {\"token\": token},\n",
    "                {\"$addToSet\": {\"postings\": doc_id}},\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "print(\"âœ… Documents updated/added and inverted index updated!\")\n",
    "print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8645ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting indexing...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 193\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_no, lines \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunked_iterator(f, BATCH_SIZE), \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    100\u001b[0m     future \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39msubmit(process_lines, lines)\n\u001b[1;32m--> 101\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     doc_ops \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    104\u001b[0m     index_ops \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mprocess_lines\u001b[1;34m(lines)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from preprocess import preprocess_text\n",
    "from itertools import islice\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "INPUT_FILE = \"batches/batch_1.json\"\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"ir_database\"\n",
    "\n",
    "DOC_COLLECTION = \"documents\"\n",
    "INDEX_COLLECTION = \"inverted_index_posting_list\"\n",
    "COUNTER_COLLECTION = \"counters\"\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "WORKERS = 8\n",
    "\n",
    "# -----------------------\n",
    "# MongoDB Setup\n",
    "# -----------------------\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "docs_col = db[DOC_COLLECTION]\n",
    "index_col = db[INDEX_COLLECTION]\n",
    "counter_col = db[COUNTER_COLLECTION]\n",
    "\n",
    "# -----------------------\n",
    "# Ensure counter exists\n",
    "# -----------------------\n",
    "counter_col.update_one(\n",
    "    {\"_id\": \"doc_id\"},\n",
    "    {\"$setOnInsert\": {\"seq\": 0}},\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "def get_next_doc_id():\n",
    "    doc = counter_col.find_one_and_update(\n",
    "        {\"_id\": \"doc_id\"},\n",
    "        {\"$inc\": {\"seq\": 1}},\n",
    "        return_document=True\n",
    "    )\n",
    "    return doc[\"seq\"]\n",
    "\n",
    "# -----------------------\n",
    "# Chunk reader\n",
    "# -----------------------\n",
    "def chunked_iterator(file, size):\n",
    "    while True:\n",
    "        chunk = list(islice(file, size))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "# -----------------------\n",
    "# Worker: preprocess only (CPU safe)\n",
    "# -----------------------\n",
    "def process_lines(lines):\n",
    "    batch = []\n",
    "\n",
    "    for line in lines:\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        url = record.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        title = record.get(\"title\", \"\")\n",
    "        description = record.get(\"description\", \"\")\n",
    "        keywords = \" \".join(record.get(\"keywords\", []))\n",
    "        visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "        full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "        tokens = list(set(preprocess_text(full_text)))\n",
    "\n",
    "        batch.append({\n",
    "            \"url\": url,\n",
    "            \"tokens\": tokens,\n",
    "            \"record\": record\n",
    "        })\n",
    "\n",
    "    return batch\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"ðŸš€ Starting indexing...\\n\")\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f, \\\n",
    "         ThreadPoolExecutor(max_workers=WORKERS) as pool:\n",
    "\n",
    "        for batch_no, lines in enumerate(chunked_iterator(f, BATCH_SIZE), 1):\n",
    "\n",
    "            future = pool.submit(process_lines, lines)\n",
    "            processed = future.result()\n",
    "\n",
    "            doc_ops = []\n",
    "            index_ops = []\n",
    "\n",
    "            for item in processed:\n",
    "                url = item[\"url\"]\n",
    "                tokens = item[\"tokens\"]\n",
    "                record = item[\"record\"]\n",
    "\n",
    "                existing = docs_col.find_one(\n",
    "                    {\"url\": url},\n",
    "                    {\"_id\": 1, \"content_hash\": 1, \"tokens\": 1}\n",
    "                )\n",
    "\n",
    "                # ------------------\n",
    "                # Skip unchanged docs\n",
    "                # ------------------\n",
    "                if existing and existing.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                    continue\n",
    "\n",
    "                # ------------------\n",
    "                # Assign doc_id\n",
    "                # ------------------\n",
    "                if existing:\n",
    "                    doc_id = existing[\"_id\"]\n",
    "\n",
    "                    # Remove old tokens from inverted index\n",
    "                    for old_token in existing.get(\"tokens\", []):\n",
    "                        index_ops.append(\n",
    "                            UpdateOne(\n",
    "                                {\"token\": old_token},\n",
    "                                {\"$pull\": {\"postings\": doc_id}}\n",
    "                            )\n",
    "                        )\n",
    "                else:\n",
    "                    doc_id = get_next_doc_id()\n",
    "\n",
    "                # ------------------\n",
    "                # Upsert document\n",
    "                # ------------------\n",
    "                doc_ops.append(\n",
    "                    UpdateOne(\n",
    "                        {\"_id\": doc_id},\n",
    "                        {\"$set\": {\n",
    "                            \"url\": url,\n",
    "                            \"title\": record.get(\"title\", \"\"),\n",
    "                            \"description\": record.get(\"description\", \"\"),\n",
    "                            \"keywords\": record.get(\"keywords\", []),\n",
    "                            \"visible_text\": record.get(\"visible_text\", \"\"),\n",
    "                            \"image_urls\": record.get(\"image_urls\", []),\n",
    "                            \"content_hash\": record.get(\"content_hash\"),\n",
    "                            \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                            \"depth\": record.get(\"depth\"),\n",
    "                            \"tokens\": tokens,\n",
    "                            \"internal_links\": record.get(\"internal_links\",[]),\n",
    "                        }},\n",
    "                        upsert=True\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # ------------------\n",
    "                # Update inverted index\n",
    "                # ------------------\n",
    "                for token in tokens:\n",
    "                    index_ops.append(\n",
    "                        UpdateOne(\n",
    "                            {\"token\": token},\n",
    "                            {\"$addToSet\": {\"postings\": doc_id}},\n",
    "                            upsert=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            # ------------------\n",
    "            # Bulk write\n",
    "            # ------------------\n",
    "            if doc_ops:\n",
    "                docs_col.bulk_write(doc_ops, ordered=False)\n",
    "\n",
    "            if index_ops:\n",
    "                index_col.bulk_write(index_ops, ordered=False)\n",
    "\n",
    "            print(f\"âœ… Batch {batch_no} indexed ({len(processed)} records)\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Indexing completed!\")\n",
    "    print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "    print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n",
    "\n",
    "# -----------------------\n",
    "# Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7213c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_17.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 16742\n",
      "ðŸ“š Total indexed terms: 124275\n",
      "18\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_18.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 17052\n",
      "ðŸ“š Total indexed terms: 125667\n",
      "19\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_19.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 17572\n",
      "ðŸ“š Total indexed terms: 129091\n",
      "20\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_20.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 18499\n",
      "ðŸ“š Total indexed terms: 134129\n",
      "21\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_21.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 19463\n",
      "ðŸ“š Total indexed terms: 138701\n",
      "22\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_22.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 20420\n",
      "ðŸ“š Total indexed terms: 141993\n",
      "23\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_23.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 21249\n",
      "ðŸ“š Total indexed terms: 144755\n",
      "24\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_24.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 21993\n",
      "ðŸ“š Total indexed terms: 146928\n",
      "25\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_25.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 22742\n",
      "ðŸ“š Total indexed terms: 147605\n",
      "26\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_26.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 23704\n",
      "ðŸ“š Total indexed terms: 150970\n",
      "27\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_27.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 24578\n",
      "ðŸ“š Total indexed terms: 153368\n",
      "28\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_28.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 25385\n",
      "ðŸ“š Total indexed terms: 155879\n",
      "29\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_29.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 26305\n",
      "ðŸ“š Total indexed terms: 157161\n",
      "30\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_30.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (499 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 27151\n",
      "ðŸ“š Total indexed terms: 159906\n",
      "31\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_31.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 28151\n",
      "ðŸ“š Total indexed terms: 164921\n",
      "32\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_32.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 29085\n",
      "ðŸ“š Total indexed terms: 173292\n",
      "33\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_33.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 30073\n",
      "ðŸ“š Total indexed terms: 175643\n",
      "34\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 1000 records from batches/batch_34.json\n",
      "\n",
      "âœ… Batch 1 indexed (500 records)\n",
      "âœ… Batch 2 indexed (500 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 31040\n",
      "ðŸ“š Total indexed terms: 181865\n",
      "35\n",
      "ðŸš€ Starting indexing...\n",
      "\n",
      "ðŸ“¦ Loaded 409 records from batches/batch_35.json\n",
      "\n",
      "âœ… Batch 1 indexed (409 records)\n",
      "\n",
      "ðŸŽ‰ Indexing completed!\n",
      "ðŸ“š Total documents: 31442\n",
      "ðŸ“š Total indexed terms: 183785\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from preprocess import preprocess_text\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "start=17\n",
    "end=36\n",
    "for i in range(start,end):\n",
    "    print(i)\n",
    "    INPUT_FILE = f\"batches/batch_{i}.json\"\n",
    "    MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "    DB_NAME = \"ir_db\"\n",
    "\n",
    "    DOC_COLLECTION = \"documents\"\n",
    "    INDEX_COLLECTION = \"inverted_index\"\n",
    "    COUNTER_COLLECTION = \"counters\"\n",
    "\n",
    "    BATCH_SIZE = 500\n",
    "    WORKERS = 8\n",
    "\n",
    "    # -----------------------\n",
    "    # MongoDB Setup\n",
    "    # -----------------------\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "\n",
    "    docs_col = db[DOC_COLLECTION]\n",
    "    index_col = db[INDEX_COLLECTION]\n",
    "    counter_col = db[COUNTER_COLLECTION]\n",
    "\n",
    "    # -----------------------\n",
    "    # Ensure indexes (run once safely)\n",
    "    # -----------------------\n",
    "    docs_col.create_index(\"url\", unique=True)\n",
    "    index_col.create_index(\"token\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Ensure counter exists\n",
    "    # -----------------------\n",
    "    counter_col.update_one(\n",
    "        {\"_id\": \"doc_id\"},\n",
    "        {\"$setOnInsert\": {\"seq\": 0}},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "    def get_next_doc_id():\n",
    "        doc = counter_col.find_one_and_update(\n",
    "            {\"_id\": \"doc_id\"},\n",
    "            {\"$inc\": {\"seq\": 1}},\n",
    "            return_document=True\n",
    "        )\n",
    "        return doc[\"seq\"]\n",
    "\n",
    "    # -----------------------\n",
    "    # Chunk list helper\n",
    "    # -----------------------\n",
    "    def chunk_list(data, size):\n",
    "        for i in range(0, len(data), size):\n",
    "            yield data[i:i + size]\n",
    "\n",
    "    # -----------------------\n",
    "    # Worker: preprocess records\n",
    "    # -----------------------\n",
    "    def process_records(records):\n",
    "        batch = []\n",
    "\n",
    "        for record in records:\n",
    "            url = record.get(\"url\")\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            title = record.get(\"title\", \"\")\n",
    "            description = record.get(\"description\", \"\")\n",
    "            keywords = \" \".join(record.get(\"keywords\", []))\n",
    "            visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "            full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "            tokens = list(set(preprocess_text(full_text)))\n",
    "\n",
    "            batch.append({\n",
    "                \"url\": url,\n",
    "                \"tokens\": tokens,\n",
    "                \"record\": record\n",
    "            })\n",
    "\n",
    "        return batch\n",
    "\n",
    "    # -----------------------\n",
    "    # Main pipeline\n",
    "    # -----------------------\n",
    "    def main():\n",
    "        print(\"ðŸš€ Starting indexing...\\n\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Load JSON batch file\n",
    "        # -----------------------\n",
    "        with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_records = json.load(f)\n",
    "\n",
    "        print(f\"ðŸ“¦ Loaded {len(all_records)} records from {INPUT_FILE}\\n\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Process in batches\n",
    "        # -----------------------\n",
    "        with ThreadPoolExecutor(max_workers=WORKERS) as pool:\n",
    "\n",
    "            for batch_no, records in enumerate(chunk_list(all_records, BATCH_SIZE), 1):\n",
    "\n",
    "                # Optional: Deduplicate URLs inside this batch\n",
    "                unique = {}\n",
    "                for r in records:\n",
    "                    if \"url\" in r:\n",
    "                        unique[r[\"url\"]] = r\n",
    "                records = list(unique.values())\n",
    "\n",
    "                future = pool.submit(process_records, records)\n",
    "                processed = future.result()\n",
    "\n",
    "                doc_ops = []\n",
    "                index_ops = []\n",
    "\n",
    "                for item in processed:\n",
    "                    url = item[\"url\"]\n",
    "                    tokens = item[\"tokens\"]\n",
    "                    record = item[\"record\"]\n",
    "\n",
    "                    existing = docs_col.find_one(\n",
    "                        {\"url\": url},\n",
    "                        {\"_id\": 1, \"content_hash\": 1, \"tokens\": 1}\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Skip unchanged docs\n",
    "                    # ------------------\n",
    "                    if existing and existing.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                        continue\n",
    "\n",
    "                    # ------------------\n",
    "                    # Assign doc_id\n",
    "                    # ------------------\n",
    "                    if existing:\n",
    "                        doc_id = existing[\"_id\"]\n",
    "\n",
    "                        # Remove old tokens from inverted index\n",
    "                        for old_token in existing.get(\"tokens\", []):\n",
    "                            index_ops.append(\n",
    "                                UpdateOne(\n",
    "                                    {\"token\": old_token},\n",
    "                                    {\"$pull\": {\"postings\": doc_id}}\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        doc_id = get_next_doc_id()\n",
    "\n",
    "                    # ------------------\n",
    "                    # âœ… FIXED: Upsert document using URL\n",
    "                    # ------------------\n",
    "                    doc_ops.append(\n",
    "                        UpdateOne(\n",
    "                            {\"url\": url},     # âœ… match by URL (not _id)\n",
    "                            {\"$set\": {\n",
    "                                \"_id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"title\": record.get(\"title\", \"\"),\n",
    "                                \"description\": record.get(\"description\", \"\"),\n",
    "                                \"keywords\": record.get(\"keywords\", []),\n",
    "                                \"visible_text\": record.get(\"visible_text\", \"\"),\n",
    "                                \"image_urls\": record.get(\"image_urls\", []),\n",
    "                                \"content_hash\": record.get(\"content_hash\"),\n",
    "                                \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                                \"depth\": record.get(\"depth\"),\n",
    "                                \"tokens\": tokens,\n",
    "                                \"internal_links\": record.get(\"internal_links\", []),\n",
    "                            }},\n",
    "                            upsert=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Update inverted index\n",
    "                    # ------------------\n",
    "                    for token in tokens:\n",
    "                        index_ops.append(\n",
    "                            UpdateOne(\n",
    "                                {\"token\": token},\n",
    "                                {\"$addToSet\": {\"postings\": doc_id}},\n",
    "                                upsert=True\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # ------------------\n",
    "                # Bulk write to MongoDB\n",
    "                # ------------------\n",
    "                if doc_ops:\n",
    "                    docs_col.bulk_write(doc_ops, ordered=False)\n",
    "\n",
    "                if index_ops:\n",
    "                    index_col.bulk_write(index_ops, ordered=False)\n",
    "\n",
    "                print(f\"âœ… Batch {batch_no} indexed ({len(processed)} records)\")\n",
    "\n",
    "        print(\"\\nðŸŽ‰ Indexing completed!\")\n",
    "        print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "        print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n",
    "\n",
    "    # -----------------------\n",
    "    # Run\n",
    "    # -----------------------\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165854f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved batches/batch_1.json (1000 records)\n",
      "âœ… Saved batches/batch_2.json (1000 records)\n",
      "âœ… Saved batches/batch_3.json (1000 records)\n",
      "âœ… Saved batches/batch_4.json (1000 records)\n",
      "âœ… Saved batches/batch_5.json (1000 records)\n",
      "âœ… Saved batches/batch_6.json (1000 records)\n",
      "âœ… Saved batches/batch_7.json (1000 records)\n",
      "âœ… Saved batches/batch_8.json (1000 records)\n",
      "âœ… Saved batches/batch_9.json (1000 records)\n",
      "âœ… Saved batches/batch_10.json (1000 records)\n",
      "âœ… Saved batches/batch_11.json (1000 records)\n",
      "âœ… Saved batches/batch_12.json (1000 records)\n",
      "âœ… Saved batches/batch_13.json (1000 records)\n",
      "âœ… Saved batches/batch_14.json (1000 records)\n",
      "âœ… Saved batches/batch_15.json (1000 records)\n",
      "âœ… Saved batches/batch_16.json (1000 records)\n",
      "âœ… Saved batches/batch_17.json (1000 records)\n",
      "âœ… Saved batches/batch_18.json (1000 records)\n",
      "âœ… Saved batches/batch_19.json (1000 records)\n",
      "âœ… Saved batches/batch_20.json (1000 records)\n",
      "âœ… Saved batches/batch_21.json (1000 records)\n",
      "âœ… Saved batches/batch_22.json (1000 records)\n",
      "âœ… Saved batches/batch_23.json (1000 records)\n",
      "âœ… Saved batches/batch_24.json (1000 records)\n",
      "âœ… Saved batches/batch_25.json (1000 records)\n",
      "âœ… Saved batches/batch_26.json (1000 records)\n",
      "âœ… Saved batches/batch_27.json (1000 records)\n",
      "âœ… Saved batches/batch_28.json (1000 records)\n",
      "âœ… Saved batches/batch_29.json (1000 records)\n",
      "âœ… Saved batches/batch_30.json (1000 records)\n",
      "âœ… Saved batches/batch_31.json (1000 records)\n",
      "âœ… Saved batches/batch_32.json (1000 records)\n",
      "âœ… Saved batches/batch_33.json (1000 records)\n",
      "âœ… Saved batches/batch_34.json (1000 records)\n",
      "âœ… Saved batches/batch_35.json (409 records)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"search_index.jsonl\"\n",
    "OUTPUT_DIR = \"batches\"\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "batch = []\n",
    "batch_id = 1\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            batch.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âŒ Skipping invalid JSON at line {line_num}\")\n",
    "            continue\n",
    "\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            output_path = f\"{OUTPUT_DIR}/batch_{batch_id}.json\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                json.dump(batch, out, indent=2)\n",
    "\n",
    "            print(f\"âœ… Saved {output_path} ({len(batch)} records)\")\n",
    "            batch.clear()\n",
    "            batch_id += 1\n",
    "\n",
    "# Save remaining records\n",
    "if batch:\n",
    "    output_path = f\"{OUTPUT_DIR}/batch_{batch_id}.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        json.dump(batch, out, indent=2)\n",
    "\n",
    "    print(f\"âœ… Saved {output_path} ({len(batch)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cfe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting indexing...\n",
      "\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_17.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_18.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_19.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (499 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_20.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_21.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (500 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_22.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n",
      "âœ… Batch 2 indexed (499 docs)\n",
      "\n",
      "ðŸ“‚ Processing: batches/batch_23.json\n",
      "ðŸ“¦ Loaded 1000 records\n",
      "âœ… Batch 1 indexed (500 docs)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from preprocess import preprocess_text\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "start = 17\n",
    "end = 36\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"ir_db\"\n",
    "\n",
    "DOC_COLLECTION = \"documents_1\"\n",
    "INDEX_COLLECTION = \"inverted_index_tf\"\n",
    "COUNTER_COLLECTION = \"counters_1\"\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "WORKERS = 8\n",
    "\n",
    "# -----------------------\n",
    "# MongoDB Setup\n",
    "# -----------------------\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "docs_col = db[DOC_COLLECTION]\n",
    "index_col = db[INDEX_COLLECTION]\n",
    "counter_col = db[COUNTER_COLLECTION]\n",
    "\n",
    "# -----------------------\n",
    "# Ensure indexes (run once safely)\n",
    "# -----------------------\n",
    "docs_col.create_index(\"url\", unique=True)\n",
    "index_col.create_index(\"token\")\n",
    "\n",
    "# -----------------------\n",
    "# Ensure counter exists\n",
    "# -----------------------\n",
    "counter_col.update_one(\n",
    "    {\"_id\": \"doc_id\"},\n",
    "    {\"$setOnInsert\": {\"seq\": 0}},\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "def get_next_doc_id():\n",
    "    doc = counter_col.find_one_and_update(\n",
    "        {\"_id\": \"doc_id\"},\n",
    "        {\"$inc\": {\"seq\": 1}},\n",
    "        return_document=True\n",
    "    )\n",
    "    return doc[\"seq\"]\n",
    "\n",
    "# -----------------------\n",
    "# Chunk list helper\n",
    "# -----------------------\n",
    "def chunk_list(data, size):\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i + size]\n",
    "\n",
    "# -----------------------\n",
    "# Worker: preprocess records\n",
    "# -----------------------\n",
    "def process_records(records):\n",
    "    batch = []\n",
    "\n",
    "    for record in records:\n",
    "        url = record.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        title = record.get(\"title\", \"\")\n",
    "        description = record.get(\"description\", \"\")\n",
    "        keywords = \" \".join(record.get(\"keywords\", []))\n",
    "        visible_text = record.get(\"visible_text\", \"\")\n",
    "\n",
    "        full_text = f\"{title} {description} {keywords} {visible_text}\"\n",
    "\n",
    "        token_list = preprocess_text(full_text)     # keep duplicates\n",
    "        tf_map = Counter(token_list)                # count TF\n",
    "\n",
    "        batch.append({\n",
    "            \"url\": url,\n",
    "            \"tf_map\": tf_map,\n",
    "            \"record\": record\n",
    "        })\n",
    "\n",
    "    return batch\n",
    "\n",
    "# -----------------------\n",
    "# Main pipeline\n",
    "# -----------------------\n",
    "def main():\n",
    "\n",
    "    print(\"ðŸš€ Starting indexing...\\n\")\n",
    "\n",
    "    for i in range(start, end):\n",
    "        INPUT_FILE = f\"batches/batch_{i}.json\"\n",
    "        print(f\"\\nðŸ“‚ Processing: {INPUT_FILE}\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Load JSON batch file\n",
    "        # -----------------------\n",
    "        with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_records = json.load(f)\n",
    "\n",
    "        print(f\"ðŸ“¦ Loaded {len(all_records)} records\")\n",
    "\n",
    "        # -----------------------\n",
    "        # Process in batches\n",
    "        # -----------------------\n",
    "        with ThreadPoolExecutor(max_workers=WORKERS) as pool:\n",
    "\n",
    "            for batch_no, records in enumerate(chunk_list(all_records, BATCH_SIZE), 1):\n",
    "\n",
    "                # Deduplicate URLs inside batch\n",
    "                unique = {}\n",
    "                for r in records:\n",
    "                    if \"url\" in r:\n",
    "                        unique[r[\"url\"]] = r\n",
    "                records = list(unique.values())\n",
    "\n",
    "                future = pool.submit(process_records, records)\n",
    "                processed = future.result()\n",
    "\n",
    "                doc_ops = []\n",
    "                index_ops = []\n",
    "\n",
    "                for item in processed:\n",
    "                    url = item[\"url\"]\n",
    "                    tf_map = item[\"tf_map\"]\n",
    "                    tokens = list(tf_map.keys())\n",
    "                    record = item[\"record\"]\n",
    "\n",
    "                    existing = docs_col.find_one(\n",
    "                        {\"url\": url},\n",
    "                        {\"_id\": 1, \"content_hash\": 1, \"tokens\": 1}\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Skip unchanged docs\n",
    "                    # ------------------\n",
    "                    if existing and existing.get(\"content_hash\") == record.get(\"content_hash\"):\n",
    "                        continue\n",
    "\n",
    "                    # ------------------\n",
    "                    # Assign doc_id\n",
    "                    # ------------------\n",
    "                    if existing:\n",
    "                        doc_id = existing[\"_id\"]\n",
    "\n",
    "                        # Remove old postings from inverted index\n",
    "                        for old_token in existing.get(\"tokens\", []):\n",
    "                            index_ops.append(\n",
    "                                UpdateOne(\n",
    "                                    {\"token\": old_token},\n",
    "                                    {\"$pull\": {\"postings\": {\"doc_id\": doc_id}}}\n",
    "                                )\n",
    "                            )\n",
    "                    else:\n",
    "                        doc_id = get_next_doc_id()\n",
    "\n",
    "                    # ------------------\n",
    "                    # Upsert document\n",
    "                    # ------------------\n",
    "                    doc_ops.append(\n",
    "                        UpdateOne(\n",
    "                            {\"url\": url},\n",
    "                            {\"$set\": {\n",
    "                                \"_id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"title\": record.get(\"title\", \"\"),\n",
    "                                \"description\": record.get(\"description\", \"\"),\n",
    "                                \"keywords\": record.get(\"keywords\", []),\n",
    "                                \"visible_text\": record.get(\"visible_text\", \"\"),\n",
    "                                \"image_urls\": record.get(\"image_urls\", []),\n",
    "                                \"content_hash\": record.get(\"content_hash\"),\n",
    "                                \"crawled_at\": record.get(\"crawled_at\"),\n",
    "                                \"depth\": record.get(\"depth\"),\n",
    "                                \"tokens\": tokens,          # unique tokens only\n",
    "                                \"internal_links\": record.get(\"internal_links\", []),\n",
    "                            }},\n",
    "                            upsert=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # ------------------\n",
    "                    # Update inverted index with TF\n",
    "                    # ------------------\n",
    "                    for token, tf in tf_map.items():\n",
    "                        index_ops.append(\n",
    "                            UpdateOne(\n",
    "                                {\"token\": token},\n",
    "                                {\"$addToSet\": {\n",
    "                                    \"postings\": {\n",
    "                                        \"doc_id\": doc_id,\n",
    "                                        \"tf\": tf\n",
    "                                    }\n",
    "                                }},\n",
    "                                upsert=True\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # ------------------\n",
    "                # Bulk write to MongoDB\n",
    "                # ------------------\n",
    "                if doc_ops:\n",
    "                    docs_col.bulk_write(doc_ops, ordered=False)\n",
    "\n",
    "                if index_ops:\n",
    "                    index_col.bulk_write(index_ops, ordered=False)\n",
    "\n",
    "                print(f\"âœ… Batch {batch_no} indexed ({len(processed)} docs)\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Indexing completed!\")\n",
    "    print(\"ðŸ“š Total documents:\", docs_col.count_documents({}))\n",
    "    print(\"ðŸ“š Total indexed terms:\", index_col.count_documents({}))\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f442a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
